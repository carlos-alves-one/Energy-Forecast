{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-Energy-Comp/blob/main/enefit_lgbm_baseline_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Necessary Libraries and Packages"
      ],
      "metadata": {
        "id": "7FXkM_mYjAHG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOov_425imOY"
      },
      "outputs": [],
      "source": [
        "import os  # Import the os module for interacting with the operating system\n",
        "import gc  # Import the garbage collector module for memory management\n",
        "\n",
        "import numpy as np               # Import numpy for numerical operations and array handling\n",
        "import pandas as pd              # Import pandas for data manipulation and analysis\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for creating static, animated, and interactive visualizations\n",
        "\n",
        "import seaborn as sns  # Import seaborn for statistical data visualization\n",
        "\n",
        "from sklearn.model_selection import cross_val_score  # Import cross_val_score for cross-validation of models\n",
        "\n",
        "import xgboost as xgb   # Import XGBoost for gradient boosting framework\n",
        "import lightgbm as lgb  # Import LightGBM for gradient boosting framework\n",
        "import catboost as cb   # Import CatBoost for gradient boosting on decision trees\n",
        "\n",
        "import optuna  # Import Optuna for hyperparameter optimization\n",
        "import shap    # Import SHAP for explaining the output of machine learning models\n",
        "\n",
        "from datetime import datetime  # Import datetime for handling date and time\n",
        "import pytz                    # Import pytz for handling timezone information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Data Files"
      ],
      "metadata": {
        "id": "eRB1xUn9lJyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating through all files in the '/kaggle/input' directory and printing their full paths\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "z21nB-SqlKWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "L2Hu5VxGlUr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/kaggle/input/predict-energy-behavior-of-prosumers\""
      ],
      "metadata": {
        "id": "4vxLsCSuCoiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset Train"
      ],
      "metadata": {
        "id": "BmyLaq_2Cz_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(os.path.join(data_dir, \"train.csv\"))"
      ],
      "metadata": {
        "id": "08-0-VuEDj6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Information\n"
      ],
      "metadata": {
        "id": "z87AiWdlE4e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first few rows, data types, and summary statistics\n",
        "print(df_train.head(5).T)\n",
        "print(df_train.info())\n",
        "print(df.describe().T)\n"
      ],
      "metadata": {
        "id": "xHbIYFcqE-gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# df_gprice   = pd.read_csv(os.path.join(data_dir, \"gas_prices.csv\"))\n",
        "# df_eprice   = pd.read_csv(os.path.join(data_dir, \"electricity_prices.csv\"))\n",
        "# df_client   = pd.read_csv(os.path.join(data_dir, \"client.csv\"))\n",
        "# df_weather  = pd.read_csv(os.path.join(data_dir, \"forecast_weather.csv\"))\n",
        "# df_hweather = pd.read_csv(os.path.join(data_dir, \"historical_weather.csv\"))\n",
        "\n",
        "df_train[\"datetime\"] = pd.to_datetime(df_train[\"datetime\"])\n",
        "\n",
        "df_train[\"month\"]   = df_train[\"datetime\"].dt.month\n",
        "df_train[\"day\"]     = df_train[\"datetime\"].dt.day\n",
        "df_train[\"weekday\"] = df_train[\"datetime\"].dt.weekday\n",
        "df_train[\"hour\"]    = df_train[\"datetime\"].dt.hour\n",
        "\n",
        "df_train[\"county\"]         = df_train[\"county\"].astype(\"category\")\n",
        "df_train[\"is_business\"]    = df_train[\"is_business\"].astype(\"category\")\n",
        "df_train[\"product_type\"]   = df_train[\"product_type\"].astype(\"category\")\n",
        "df_train[\"is_consumption\"] = df_train[\"is_consumption\"].astype(\"category\")\n",
        "\n",
        "df_train = df_train.set_index([\"row_id\", \"datetime\"])\n",
        "df_train = df_train.drop(columns=[\"prediction_unit_id\", \"data_block_id\"])\n",
        "\n",
        "df_train = df_train.dropna(subset=[\"target\"])\n"
      ],
      "metadata": {
        "id": "sSHFDofglbCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declare Class MonthlyKFold\n",
        "\n",
        "Custom cross-validator designed for time series data, where the data is split based on unique monthly time steps."
      ],
      "metadata": {
        "id": "4iyjO37kl4FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MonthlyKFold:\n",
        "    def __init__(self, n_splits=3):\n",
        "        self.n_splits = n_splits  # Initialize the class with n_splits, default is 3\n",
        "\n",
        "    def split(self, X, y, groups=None):\n",
        "        # Calculate monthly timesteps from the second level of the multi-index of X\n",
        "        dates = X.index.get_level_values(1) - pd.offsets.MonthBegin(1, normalize=True)\n",
        "        # Sort and list unique dates for splitting\n",
        "        timesteps = sorted(dates.unique().tolist())\n",
        "        # Reset index of X for easier indexing later\n",
        "        X = X.reset_index().copy()\n",
        "\n",
        "        # Iterate over the last n_splits months\n",
        "        for t in timesteps[-self.n_splits:]:\n",
        "            # Define training set indices (all data before the current timestep)\n",
        "            idx_train = X[dates < t].index\n",
        "            # Define test set indices (data from the current timestep)\n",
        "            idx_test = X[dates == t].index\n",
        "\n",
        "            # Yield indices for the training and test sets\n",
        "            yield idx_train, idx_test\n",
        "\n",
        "    def get_n_splits(self, X, y, groups=None):\n",
        "        # Return the number of split iterations in the cross-validator\n",
        "        return self.n_splits\n"
      ],
      "metadata": {
        "id": "9ryZmL0BmNKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "Designed for preprocessing a DataFrame. It handles categorical data conversion, datetime feature extraction, and data cleaning."
      ],
      "metadata": {
        "id": "o4N5Af1Nm6hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_eng(df):\n",
        "    # Convert specified columns to 'category' data type for efficient storage and computation\n",
        "    categorical_columns = [\"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
        "    for col in categorical_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    # Convert 'currently_scored' to an integer if it's a boolean, or to a datetime object if not\n",
        "    if 'currently_scored' in df.columns:\n",
        "        if pd.api.types.is_bool_dtype(df['currently_scored']):\n",
        "            df['currently_scored'] = df['currently_scored'].astype(int)\n",
        "        else:\n",
        "            df['currently_scored'] = pd.to_datetime(df['currently_scored'], errors='coerce')\n",
        "\n",
        "    # Extract datetime features from 'prediction_datetime' if present\n",
        "    if 'prediction_datetime' in df.columns:\n",
        "        df['prediction_datetime'] = pd.to_datetime(df['prediction_datetime'], errors='coerce')\n",
        "        df['month'] = df['prediction_datetime'].dt.month      # Extract month\n",
        "        df['day'] = df['prediction_datetime'].dt.day          # Extract day\n",
        "        df['weekday'] = df['prediction_datetime'].dt.weekday  # Extract day of the week\n",
        "        df['hour'] = df['prediction_datetime'].dt.hour        # Extract hour\n",
        "\n",
        "    # Set 'row_id' as the index if it exists, useful for identification and lookup\n",
        "    if 'row_id' in df.columns:\n",
        "        df = df.set_index('row_id')\n",
        "\n",
        "    # Drop columns that are no longer needed\n",
        "    df = df.drop(columns=['currently_scored', 'prediction_datetime', 'prediction_unit_id'], errors='ignore')\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "nrvEzCEQnRQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declare Function lgb_objective\n",
        "\n",
        "Designed to optimize hyperparameters for a LightGBM regressor using Optuna, a hyperparameter optimization framework. It utilizes a custom cross-validation strategy and aims to minimize the mean absolute error of the model predictions."
      ],
      "metadata": {
        "id": "Ry3VmF_Zol0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_objective(trial):\n",
        "    # Define hyperparameters for the LightGBM model with ranges to be optimized by Optuna\n",
        "    params = {\n",
        "        'n_iter'           : 200,  # Number of boosting iterations\n",
        "        'verbosity'        : -1,   # Control verbosity; -1 means no output\n",
        "        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1.0),         # Fraction of features to be considered for each split\n",
        "        'colsample_bynode' : trial.suggest_float('colsample_bynode', 0.5, 1.0),         # Fraction of features for each node\n",
        "        'max_depth'        : trial.suggest_int('max_depth', 3, 10),                     # Maximum depth of the tree\n",
        "        'learning_rate'    : trial.suggest_float('learning_rate', 0.01, 0.1, log=True), # Learning rate for gradient descent\n",
        "        'lambda_l1'        : trial.suggest_float('lambda_l1', 1e-2, 10.0),   # L1 regularization term\n",
        "        'lambda_l2'        : trial.suggest_float('lambda_l2', 1e-2, 10.0),   # L2 regularization term\n",
        "        'num_leaves'       : trial.suggest_int('num_leaves', 16, 256),       # Maximum number of leaves in one tree\n",
        "        'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 4, 256),  # Minimum data in one leaf\n",
        "    }\n",
        "\n",
        "    # Initialize the LightGBM regressor with the defined parameters\n",
        "    model  = lgb.LGBMRegressor(**params)\n",
        "    # Separate features (X) and target (y) from the training dataset\n",
        "    X, y   = df_train.drop(columns=[\"target\"]), df_train[\"target\"]\n",
        "    # Initialize a custom cross-validation strategy, MonthlyKFold\n",
        "    cv = MonthlyKFold(3)\n",
        "    # Perform cross-validation and compute the negative mean absolute error\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
        "\n",
        "    # Return the average negative mean absolute error as the optimization objective\n",
        "    return -1 * np.mean(scores)\n"
      ],
      "metadata": {
        "id": "NV0P9TYtomL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the LightGBM regressor model\n",
        "\n",
        "With a set of optimized hyperparameters:"
      ],
      "metadata": {
        "id": "xXT176tapVvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {\n",
        "    'n_iter': 300,            # Increased to allow more thorough hyperparameter search\n",
        "    'verbosity': 1,           # Set to 1 for standard output of messages\n",
        "    'colsample_bytree': 0.8,  # Reduced to introduce more randomness and prevent overfitting\n",
        "    'colsample_bynode': 0.8,  # Same reasoning as colsample_bytree\n",
        "    'max_depth': 6,           # Reduced to combat potential overfitting\n",
        "    'learning_rate': 0.1,     # Slightly increased for potentially better convergence\n",
        "    'lambda_l1': 1.0,         # Increased for stronger L1 regularization to prevent overfitting\n",
        "    'lambda_l2': 1.0,         # Increased for stronger L2 regularization to prevent overfitting\n",
        "    'num_leaves': 120,        # Reduced to prevent overfitting, especially in LightGBM\n",
        "    'min_data_in_leaf': 20    # Increased to ensure a reasonable number of samples per leaf\n",
        "}\n",
        "\n",
        "# Initialize the LightGBM model with the optimized parameters\n",
        "model = lgb.LGBMRegressor(**best_params)\n"
      ],
      "metadata": {
        "id": "9UCBYRdZpWPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the LightGBM model\n",
        "\n",
        "Using the Training Data"
      ],
      "metadata": {
        "id": "o3x_-AkGqRzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and target variable from the training dataset\n",
        "X_train, y_train = df_train.drop(columns=[\"target\"]), df_train[\"target\"]\n",
        "\n",
        "# Fit the LightGBM model to the training data\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "jEklwV4AqSMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit API"
      ],
      "metadata": {
        "id": "G0v3ApvwrE3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the 'enefit' library, a custom or specific library for a certain task or competition\n",
        "import enefit\n",
        "\n",
        "# Initializing an environment using a function from the 'enefit' library. This environment is used for simulation or testing.\n",
        "env = enefit.make_env()\n",
        "\n",
        "# Creating an iterator for the test set. Used in scenarios to provided a streaming test set, to make predictions iteratively.\n",
        "iter_test = env.iter_test()\n",
        "\n",
        "# Initialize a list to store all predictions\n",
        "all_predictions = []\n",
        "\n",
        "# Flag to indicate if the shape and column names have been printed\n",
        "printed_info = False\n",
        "\n",
        "for (test, revealed_targets, client, historical_weather,\n",
        "     forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
        "\n",
        "    # Apply feature engineering\n",
        "    X_test = feature_eng(test)\n",
        "\n",
        "    # Print shape and column names only once\n",
        "    if not printed_info:\n",
        "        print(\"\\n-> Shape of X_test:\", X_test.shape)\n",
        "        print(\"\\n-> Column names in X_test:\")\n",
        "        for column in X_test.columns:\n",
        "            print(column)\n",
        "        printed_info = True\n",
        "\n",
        "    try:\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred = y_pred.clip(0)\n",
        "\n",
        "        # Assign predictions to sample_prediction\n",
        "        sample_prediction['target'] = y_pred\n",
        "\n",
        "        # Store the predictions for later use\n",
        "        all_predictions.append(y_pred)\n",
        "\n",
        "        # Make submission\n",
        "        env.predict(sample_prediction)\n",
        "\n",
        "    except Exception as e:  # Catching a broader range of exceptions\n",
        "        print(\"\\n-> Error in prediction or data processing:\", e)\n",
        "\n",
        "        # Assuming 'model' is your trained model and 'X_train' is the training data\n",
        "        if hasattr(model, 'n_features_'):\n",
        "            print(f\"\\n-> Number of features used in the model: {model.n_features_}\")\n",
        "\n",
        "            # If X_train is a pandas DataFrame, we can get the feature names directly\n",
        "            if isinstance(X_train, pd.DataFrame):\n",
        "                feature_names = X_train.columns\n",
        "                print(\"\\n-> Feature names:\")\n",
        "                for name in feature_names:\n",
        "                    print(name)\n",
        "            else:\n",
        "                print(\"\\nFeature names are not available as X_train is not a pandas DataFrame.\")\n",
        "        else:\n",
        "            print(\"The model does not have the attribute 'n_features_'.\")\n",
        "\n",
        "        break  # Exit the loop if there's an error\n",
        "\n",
        "# Get the current time in GMT\n",
        "gmt_time = datetime.now(pytz.timezone('GMT'))\n",
        "\n",
        "# Formats the GMT time into a string with the format \"hour:minute:second AM/PM month/day/year GMT\"\n",
        "formatted_time = gmt_time.strftime('%I:%M:%S %p %m/%d/%Y GMT')\n",
        "print(\"\\n-> Model Submission was Made Successfully at\", formatted_time)\n",
        "\n",
        "# After the loop, convert the list of predictions to a DataFrame\n",
        "df_predictions = pd.DataFrame(all_predictions)\n",
        "\n",
        "# Optionally name the columns\n",
        "column_names = [f'Prediction {j}' for j in range(df_predictions.shape[1])]\n",
        "df_predictions.columns = column_names\n",
        "\n",
        "# Set the index to represent each iteration\n",
        "df_predictions.index = [f'Iteration {i}' for i in range(df_predictions.shape[0])]\n",
        "\n",
        "# Print the DataFrame\n",
        "print(\"\\n-> All Predictions of The Model:\\n\")\n",
        "print(df_predictions)\n"
      ],
      "metadata": {
        "id": "TUDnod6qrFd0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}