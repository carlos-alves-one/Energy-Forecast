{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgwsvpexi3V99H3MdlOCSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-Energy-Comp/blob/main/enefit_lgbm_baseline_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries and Packages"
      ],
      "metadata": {
        "id": "7FXkM_mYjAHG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOov_425imOY"
      },
      "outputs": [],
      "source": [
        "import os  # Import the os module for interacting with the operating system\n",
        "import gc  # Import the garbage collector module for memory management\n",
        "\n",
        "import numpy as np               # Import numpy for numerical operations and array handling\n",
        "import pandas as pd              # Import pandas for data manipulation and analysis\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for creating static, animated, and interactive visualizations\n",
        "\n",
        "import seaborn as sns  # Import seaborn for statistical data visualization\n",
        "\n",
        "from sklearn.model_selection import cross_val_score  # Import cross_val_score for cross-validation of models\n",
        "\n",
        "import xgboost as xgb   # Import XGBoost for gradient boosting framework\n",
        "import lightgbm as lgb  # Import LightGBM for gradient boosting framework\n",
        "import catboost as cb   # Import CatBoost for gradient boosting on decision trees\n",
        "\n",
        "import optuna  # Import Optuna for hyperparameter optimization\n",
        "import shap    # Import SHAP for explaining the output of machine learning models\n",
        "\n",
        "from datetime import datetime  # Import datetime for handling date and time\n",
        "import pytz                    # Import pytz for handling timezone information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Data Files"
      ],
      "metadata": {
        "id": "eRB1xUn9lJyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating through all files in the '/kaggle/input' directory and printing their full paths\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        ""
      ],
      "metadata": {
        "id": "z21nB-SqlKWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "L2Hu5VxGlUr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
        "\n",
        "df_train = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "# df_gprice = pd.read_csv(os.path.join(data_dir, \"gas_prices.csv\"))\n",
        "# df_eprice = pd.read_csv(os.path.join(data_dir, \"electricity_prices.csv\"))\n",
        "# df_client = pd.read_csv(os.path.join(data_dir, \"client.csv\"))\n",
        "# df_weather = pd.read_csv(os.path.join(data_dir, \"forecast_weather.csv\"))\n",
        "# df_hweather = pd.read_csv(os.path.join(data_dir, \"historical_weather.csv\"))\n",
        "\n",
        "df_train[\"datetime\"] = pd.to_datetime(df_train[\"datetime\"])\n",
        "\n",
        "df_train[\"month\"]   = df_train[\"datetime\"].dt.month\n",
        "df_train[\"day\"]     = df_train[\"datetime\"].dt.day\n",
        "df_train[\"weekday\"] = df_train[\"datetime\"].dt.weekday\n",
        "df_train[\"hour\"]    = df_train[\"datetime\"].dt.hour\n",
        "\n",
        "df_train[\"county\"]         = df_train[\"county\"].astype(\"category\")\n",
        "df_train[\"is_business\"]    = df_train[\"is_business\"].astype(\"category\")\n",
        "df_train[\"product_type\"]   = df_train[\"product_type\"].astype(\"category\")\n",
        "df_train[\"is_consumption\"] = df_train[\"is_consumption\"].astype(\"category\")\n",
        "\n",
        "df_train = df_train.set_index([\"row_id\", \"datetime\"])\n",
        "df_train = df_train.drop(columns=[\"prediction_unit_id\", \"data_block_id\"])\n",
        "\n",
        "df_train = df_train.dropna(subset=[\"target\"])\n"
      ],
      "metadata": {
        "id": "sSHFDofglbCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declare Class MonthlyKFold\n",
        "\n",
        "Custom cross-validator designed for time series data, where the data is split based on unique monthly time steps."
      ],
      "metadata": {
        "id": "4iyjO37kl4FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MonthlyKFold:\n",
        "    def __init__(self, n_splits=3):\n",
        "        self.n_splits = n_splits  # Initialize the class with n_splits, default is 3\n",
        "\n",
        "    def split(self, X, y, groups=None):\n",
        "        # Calculate monthly timesteps from the second level of the multi-index of X\n",
        "        dates = X.index.get_level_values(1) - pd.offsets.MonthBegin(1, normalize=True)\n",
        "        # Sort and list unique dates for splitting\n",
        "        timesteps = sorted(dates.unique().tolist())\n",
        "        # Reset index of X for easier indexing later\n",
        "        X = X.reset_index().copy()\n",
        "\n",
        "        # Iterate over the last n_splits months\n",
        "        for t in timesteps[-self.n_splits:]:\n",
        "            # Define training set indices (all data before the current timestep)\n",
        "            idx_train = X[dates < t].index\n",
        "            # Define test set indices (data from the current timestep)\n",
        "            idx_test = X[dates == t].index\n",
        "\n",
        "            # Yield indices for the training and test sets\n",
        "            yield idx_train, idx_test\n",
        "\n",
        "    def get_n_splits(self, X, y, groups=None):\n",
        "        # Return the number of split iterations in the cross-validator\n",
        "        return self.n_splits\n"
      ],
      "metadata": {
        "id": "9ryZmL0BmNKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "Designed for preprocessing a DataFrame. It handles categorical data conversion, datetime feature extraction, and data cleaning."
      ],
      "metadata": {
        "id": "o4N5Af1Nm6hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_eng(df):\n",
        "    # Convert specified columns to 'category' data type for efficient storage and computation\n",
        "    categorical_columns = [\"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
        "    for col in categorical_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    # Convert 'currently_scored' to an integer if it's a boolean, or to a datetime object if not\n",
        "    if 'currently_scored' in df.columns:\n",
        "        if pd.api.types.is_bool_dtype(df['currently_scored']):\n",
        "            df['currently_scored'] = df['currently_scored'].astype(int)\n",
        "        else:\n",
        "            df['currently_scored'] = pd.to_datetime(df['currently_scored'], errors='coerce')\n",
        "\n",
        "    # Extract datetime features from 'prediction_datetime' if present\n",
        "    if 'prediction_datetime' in df.columns:\n",
        "        df['prediction_datetime'] = pd.to_datetime(df['prediction_datetime'], errors='coerce')\n",
        "        df['month'] = df['prediction_datetime'].dt.month      # Extract month\n",
        "        df['day'] = df['prediction_datetime'].dt.day          # Extract day\n",
        "        df['weekday'] = df['prediction_datetime'].dt.weekday  # Extract day of the week\n",
        "        df['hour'] = df['prediction_datetime'].dt.hour        # Extract hour\n",
        "\n",
        "    # Set 'row_id' as the index if it exists, useful for identification and lookup\n",
        "    if 'row_id' in df.columns:\n",
        "        df = df.set_index('row_id')\n",
        "\n",
        "    # Drop columns that are no longer needed\n",
        "    df = df.drop(columns=['currently_scored', 'prediction_datetime', 'prediction_unit_id'], errors='ignore')\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "nrvEzCEQnRQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}