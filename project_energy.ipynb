{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuxqrTteWis0ltB3D4385k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-Energy-Comp/blob/main/project_energy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Understanding the Problem and the Data"
      ],
      "metadata": {
        "id": "stdTozWSl-nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understand the specific problem of energy imbalance caused by prosumers and how the model can help Enefit**\n",
        "\n",
        "Specific Problem: Energy Imbalance Resulting from Prosumers\n",
        "\n",
        "The primary concern is the energy discrepancy that occurs when there is a disparity between the anticipated and the actual energy consumed or generated. The issue is worsened by prosumers contributing to the problem due to their simultaneous roles as energy consumers and producers. Their energy use and generation can be erratic, resulting in logistical and financial difficulties for energy firms like Enefit. These problems encompass the struggle to align supply and demand and the resulting expenditures from this imbalance.\n",
        "\n",
        "The Role of the Model in Facilitating Enefit\n",
        "The model aims to address these difficulties by offering precise forecasts of prosumers' energy usage and production. By doing so, the model will:\n",
        "\n",
        "1. Increase Forecasting Precision: Enhance Enefit's capacity to anticipate energy demands and production levels accurately.\n",
        "   \n",
        "2. Minimise Imbalance Expenses: Enefit can optimise its energy allocation by utilising more accurate forecasts, hence decreasing the expenses linked to energy imbalance.\n",
        "\n",
        "3. Enhance Resource Allocation Efficiency: Precise predictions will allow Enefit to distribute resources more optimally, thereby minimising waste and decreasing operational expenses.\n",
        "\n",
        "4. Enhance Strategic Decision-Making: Enefit can improve its ability to make strategic decisions on infrastructure investments and policy changes by gaining deeper insights into consumer behaviour.\n",
        "\n",
        "5. Encourage Sustainable Habits: Enefit may encourage prosumers to use renewable energy sources through efficient energy management, facilitating the shift towards more environmentally friendly energy habits.\n",
        "\n",
        "The model must incorporate multiple variables that impact consumer behaviour, such as weather patterns, past energy usage trends, pricing fluctuations, etc. The model's performance will be assessed based on its Mean Absolute Error (MAE), which requires your predictions to match the actual values to minimise the error measurement closely.\n",
        "\n",
        "The competition offers a dataset of historical meteorological data, energy pricing, and details regarding prosumer attributes. The given Python time-series API will guarantee that the model complies with the competition's specifications, including the prohibition of looking ahead in time and utilising just the available data for making predictions."
      ],
      "metadata": {
        "id": "70MefOSdnE-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Study the Data"
      ],
      "metadata": {
        "id": "zzloKS5iq9s1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Collection"
      ],
      "metadata": {
        "id": "UvEsJEcAsPRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Load the Data\n",
        "   - Connect to Google Drive to access the dataset\n",
        "   - Load the data from the provided CSV file."
      ],
      "metadata": {
        "id": "imtu4qG2sUe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports the 'drive' module from 'google.colab' and mounts the Google Drive to\n",
        "# the '/content/drive' directory in the Colab environment.\n",
        "from google.colab import drive\n",
        "\n",
        "# This function mounts Google Drive\n",
        "def mount_google_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function to mount Google Drive\n",
        "mount_google_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roqLTiEFrN5K",
        "outputId": "282b9f18-a84f-41ab-c43a-88c8fc490a0a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit tests\n",
        "import unittest\n",
        "from unittest.mock import patch\n",
        "\n",
        "class TestDriveMount(unittest.TestCase):\n",
        "    @patch('google.colab.drive.mount')\n",
        "    def test_drive_mount(self, mock_drive_mount):\n",
        "        # Mock the drive.mount function\n",
        "        mock_drive_mount.return_value = None\n",
        "\n",
        "        # Call the function we want to test\n",
        "        mount_google_drive()\n",
        "\n",
        "        # Assert that drive.mount was called with the correct arguments\n",
        "        mock_drive_mount.assert_called_once_with('/content/drive')\n",
        "\n",
        "# Running the tests\n",
        "unittest.main(argv=[''], exit=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjbPFy4BsznR",
        "outputId": "776406ce-7d5a-446f-bbb8-575127bca3b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.025s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7ad350ff1e40>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pandas library to read the data\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the os module\n",
        "import os\n",
        "\n",
        "# Function to load the dataset\n",
        "def load_dataset(path):\n",
        "    data = pd.read_csv(path)\n",
        "    return data\n",
        "\n",
        "class TestDatasetLoading(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        # Define the dataset path\n",
        "        self.dataset_path = '/content/drive/MyDrive/Glaucoma_Global_Analysis.csv'\n",
        "\n",
        "    def test_file_exists(self):\n",
        "        # Test whether the file exists\n",
        "        self.assertTrue(os.path.isfile(self.dataset_path), \"Dataset file does not exist at the specified path.\")\n",
        "\n",
        "    def test_load_dataset(self):\n",
        "        # Test loading the dataset\n",
        "        data = load_dataset(self.dataset_path)\n",
        "        self.assertIsInstance(data, pd.DataFrame, \"Loaded data is not a pandas DataFrame.\")\n",
        "\n",
        "    def test_dataset_not_empty(self):\n",
        "        # Test that the dataset is not empty\n",
        "        data = load_dataset(self.dataset_path)\n",
        "        self.assertFalse(data.empty, \"The loaded dataset is empty.\")\n",
        "\n",
        "# Running the tests\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''], exit=False)\n",
        "\n",
        "# Load the dataset and display the first 3 rows\n",
        "data = load_dataset('/content/drive/MyDrive/project_energy/train.csv')\n",
        "print(data.head(1).T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_wZvgREs18y",
        "outputId": "92225a81-4a43-43ee-b30e-eb5a2e8376ff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.023s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      0\n",
            "county                                0\n",
            "is_business                           0\n",
            "product_type                          1\n",
            "target                            0.713\n",
            "is_consumption                        0\n",
            "datetime            2021-09-01 00:00:00\n",
            "data_block_id                         0\n",
            "row_id                                0\n",
            "prediction_unit_id                    0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pandas library to read the data\n",
        "import pandas as pd\n",
        "\n",
        "# Load the training data\n",
        "train_data = data\n",
        "\n",
        "# Get the number of rows and columns\n",
        "num_rows, num_columns = train_data.shape\n",
        "\n",
        "# Format and print the numbers with dots as thousand separators\n",
        "print(\"Number of Rows.....:\", \"{:,}\".format(num_rows).replace(\",\", \".\"))\n",
        "print(\"Number of Columns..:\", \"{:,}\".format(num_columns).replace(\",\", \".\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0rNGdAjx8lb",
        "outputId": "7094703e-9ae2-4026-bbbf-ccf8837b6439"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Rows.....: 2.018.352\n",
            "Number of Columns..: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the Data"
      ],
      "metadata": {
        "id": "1xsEONfhrG98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dask's dataframe module for parallel data processing.\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# Load data into a Dask DataFrame\n",
        "# Replace 'data' with the data source\n",
        "train_data = dd.from_pandas(data, npartitions=10)  # Adjust npartitions based on your system's capabilities\n",
        "\n",
        "# Handling missing values\n",
        "# For numerical columns, fill missing values with the mean\n",
        "numerical_columns = train_data.select_dtypes(include=['number']).columns\n",
        "for col in numerical_columns:\n",
        "    train_data[col] = train_data[col].fillna(train_data[col].mean())\n",
        "\n",
        "# For categorical data, fill missing values with the mode\n",
        "categorical_columns = train_data.select_dtypes(include=['object', 'category']).columns\n",
        "for col in categorical_columns:\n",
        "    mode = train_data[col].mode().compute()[0]\n",
        "    train_data[col] = train_data[col].fillna(mode)\n",
        "\n",
        "# Handle outliers\n",
        "# Assuming 'target' column could have outliers, use IQR\n",
        "Q1 = train_data['target'].quantile(0.25).compute()\n",
        "Q3 = train_data['target'].quantile(0.75).compute()\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the outlier condition\n",
        "outlier_condition = ((train_data['target'] < (Q1 - 1.5 * IQR)) | (train_data['target'] > (Q3 + 1.5 * IQR)))\n",
        "\n",
        "# Filter out the outliers\n",
        "train_data = train_data[~outlier_condition]\n",
        "\n",
        "# Convert back to pandas DataFrame if needed (optional)\n",
        "# train_data = train_data.compute()  # Uncomment this line if to need to work with a pandas DataFrame\n"
      ],
      "metadata": {
        "id": "hxpmMv39rH2r"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Transformation"
      ],
      "metadata": {
        "id": "yvvef0zJ_ejd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the columns in the DataFrame\n",
        "print(train_data.columns)\n"
      ],
      "metadata": {
        "id": "8wQVwhD_ApNs",
        "outputId": "d675e3cb-8593-4a47-ca1c-0b2e7e2d7270",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['county', 'is_business', 'product_type', 'target', 'is_consumption',\n",
            "       'datetime', 'data_block_id', 'row_id', 'prediction_unit_id'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert boolean columns to integers\n",
        "train_data['is_business'] = train_data['is_business'].astype(int)\n",
        "train_data['is_consumption'] = train_data['is_consumption'].astype(int)\n",
        "\n",
        "!pip install dask_ml\n",
        "from dask_ml.preprocessing import OneHotEncoder\n",
        "\n",
        "# Convert 'product_type' to categorical type\n",
        "train_data['product_type'] = train_data['product_type'].astype('category')\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "from dask_ml.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# First, convert 'product_type' to categorical type\n",
        "train_data['product_type'] = train_data['product_type'].astype('category')\n",
        "\n",
        "# Use categorize() to ensure categories are known\n",
        "train_data = train_data.categorize(columns=['product_type'])\n",
        "\n",
        "# Initialize and apply OneHotEncoder\n",
        "from dask_ml.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Fit and transform the 'product_type' column\n",
        "encoded_data = encoder.fit_transform(train_data[['product_type']])\n",
        "\n",
        "# Concatenate the encoded data with the original DataFrame\n",
        "train_data = train_data.drop('product_type', axis=1)\n",
        "train_data = train_data.join(encoded_data)\n"
      ],
      "metadata": {
        "id": "pOqM_J6mI1Tz",
        "outputId": "7a2eff63-a235-49ec-c480-8bd32a7829a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask_ml in /usr/local/lib/python3.10/dist-packages (2023.3.24)\n",
            "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (2023.8.1)\n",
            "Requirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (2023.8.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.11.4)\n",
            "Requirement already satisfied: dask-glm>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (0.3.2)\n",
            "Requirement already satisfied: multipledispatch>=0.4.9 in /usr/local/lib/python3.10/dist-packages (from dask_ml) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dask_ml) (23.2)\n",
            "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from dask-glm>=0.2.0->dask_ml) (2.2.1)\n",
            "Requirement already satisfied: sparse>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from dask-glm>=0.2.0->dask_ml) (0.14.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (8.1.7)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (7.0.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (3.1.2)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (1.0.7)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (6.3.2)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (2.0.7)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.4.0->dask_ml) (3.0.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->dask_ml) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask_ml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->dask_ml) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->dask_ml) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->dask_ml) (3.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[array,dataframe]>=2.4.0->dask_ml) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask_ml) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.2->dask_ml) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "from dask_ml.model_selection import train_test_split\n",
        "from dask_ml.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Assuming train_data is already loaded as a Dask DataFrame\n",
        "\n",
        "# Drop the original datetime column\n",
        "train_data = train_data.drop('datetime', axis=1)\n",
        "\n",
        "# Select only the numeric columns for features\n",
        "X = train_data[train_data.columns.difference(['target'])]\n",
        "y = train_data['target']\n",
        "\n",
        "# Convert the Dask DataFrame to a Dask Array\n",
        "X = X.to_dask_array(lengths=True)\n",
        "y = y.to_dask_array(lengths=True)\n",
        "\n",
        "# Split the data using Dask's train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Dask-ML model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Now we can use model.predict() to make predictions on new data\n"
      ],
      "metadata": {
        "id": "caURjgB1_fSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfbZOwepMoZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}