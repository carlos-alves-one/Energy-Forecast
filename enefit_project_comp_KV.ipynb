{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 57236,
          "databundleVersionId": 7284606,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-Energy-Comp/blob/main/enefit_project_comp_KV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-12-14T21:46:21.939132Z",
          "iopub.execute_input": "2023-12-14T21:46:21.939560Z",
          "iopub.status.idle": "2023-12-14T21:46:22.000656Z",
          "shell.execute_reply.started": "2023-12-14T21:46:21.939527Z",
          "shell.execute_reply": "2023-12-14T21:46:21.999355Z"
        },
        "trusted": true,
        "id": "235YeNywUOgc",
        "outputId": "746a7495-a2ea-4ff1-9559-66367791505b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/predict-energy-behavior-of-prosumers/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/public_timeseries_testing_util.py\n/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/county_id_to_name_map.json\n/kaggle/input/predict-energy-behavior-of-prosumers/train.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/sample_submission.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/client.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/gas_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/electricity_prices.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/historical_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/revealed_targets.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/predict-energy-behavior-of-prosumers/enefit/__init__.py\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade polars"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T21:46:22.003163Z",
          "iopub.execute_input": "2023-12-14T21:46:22.003662Z",
          "iopub.status.idle": "2023-12-14T21:46:38.891176Z",
          "shell.execute_reply.started": "2023-12-14T21:46:22.003616Z",
          "shell.execute_reply": "2023-12-14T21:46:38.889310Z"
        },
        "trusted": true,
        "id": "B_iFw8TQUOge",
        "outputId": "c06da70c-2c92-4fac-b0ef-d448136b553e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: polars in /opt/conda/lib/python3.10/site-packages (0.19.19)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries and Packages"
      ],
      "metadata": {
        "id": "QztfcFCEUOge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "import enefit\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "import pickle\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T21:46:38.894008Z",
          "iopub.execute_input": "2023-12-14T21:46:38.894576Z",
          "iopub.status.idle": "2023-12-14T21:46:44.011144Z",
          "shell.execute_reply.started": "2023-12-14T21:46:38.894521Z",
          "shell.execute_reply": "2023-12-14T21:46:44.009899Z"
        },
        "trusted": true,
        "id": "nlcQF3JjUOgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "zdVo4sNsUOgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
        "\n",
        "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
        "location_cols    = ['longitude', 'latitude', 'county']\n",
        "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
        "\n",
        "save_path = None\n",
        "load_path = None\n",
        "\n",
        "df_data        = pl.read_csv(os.path.join(root, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
        "df_client      = pl.read_csv(os.path.join(root, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
        "df_gas         = pl.read_csv(os.path.join(root, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
        "df_electricity = pl.read_csv(os.path.join(root, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
        "df_forecast    = pl.read_csv(os.path.join(root, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
        "df_historical  = pl.read_csv(os.path.join(root, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
        "df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
        "df_target      = df_data.select(target_cols)\n",
        "\n",
        "schema_data        = df_data.schema\n",
        "schema_client      = df_client.schema\n",
        "schema_gas         = df_gas.schema\n",
        "schema_electricity = df_electricity.schema\n",
        "schema_forecast    = df_forecast.schema\n",
        "schema_historical  = df_historical.schema\n",
        "schema_target      = df_target.schema\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T21:46:44.014952Z",
          "iopub.execute_input": "2023-12-14T21:46:44.015506Z",
          "iopub.status.idle": "2023-12-14T21:46:49.996001Z",
          "shell.execute_reply.started": "2023-12-14T21:46:44.015455Z",
          "shell.execute_reply": "2023-12-14T21:46:49.995099Z"
        },
        "trusted": true,
        "id": "afrHa9EAUOgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "Rk5bXeQhUOgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "# Columns specifications\n",
        "data_cols = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high', 'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation', 'diffuse_radiation', 'latitude', 'longitude']\n",
        "location_cols = ['longitude', 'latitude', 'county']\n",
        "\n",
        "# Helper function to read large CSV files directly into Polars DataFrames\n",
        "def read_csv_polars(file_path, columns, batch_size=5 * 10 ** 5):  # Adjust batch_size as needed\n",
        "    return pl.read_csv(file_path, columns=columns, batch_size=batch_size)\n",
        "\n",
        "# Reading CSV files directly into Polars DataFrames\n",
        "df_data = read_csv_polars(os.path.join(root, \"train.csv\"), data_cols)\n",
        "df_client = read_csv_polars(os.path.join(root, \"client.csv\"), client_cols)\n",
        "df_gas = read_csv_polars(os.path.join(root, \"gas_prices.csv\"), gas_cols)\n",
        "df_electricity = read_csv_polars(os.path.join(root, \"electricity_prices.csv\"), electricity_cols)\n",
        "df_forecast = read_csv_polars(os.path.join(root, \"forecast_weather.csv\"), forecast_cols)\n",
        "df_historical = read_csv_polars(os.path.join(root, \"historical_weather.csv\"), historical_cols)\n",
        "df_location = read_csv_polars(os.path.join(root, \"weather_station_to_county_mapping.csv\"), location_cols)\n",
        "\n",
        "# Define a date format string compatible with your data\n",
        "date_format = \"%Y-%m-%d %H:%M:%S\"  # Adjust the format according to the data\n",
        "\n",
        "# Convert 'datetime' columns to datetime format in all dataframes\n",
        "df_data = df_data.with_columns([pl.col('datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_client = df_client.with_columns([pl.col('date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_gas = df_gas.with_columns([pl.col('forecast_date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_electricity = df_electricity.with_columns([pl.col('forecast_date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_forecast = df_forecast.with_columns([pl.col('forecast_datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_historical = df_historical.with_columns([pl.col('datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "\n",
        "# Standardize datetime precision to microseconds\n",
        "# This function is simplified assuming the datetime data is already timezone-naive\n",
        "def standardize_datetime_precision(df, datetime_columns):\n",
        "    for col in datetime_columns:\n",
        "        df = df.with_columns([pl.col(col).cast(pl.Datetime)])\n",
        "    return df\n",
        "\n",
        "datetime_columns_data = ['datetime']\n",
        "datetime_columns_client = ['date']\n",
        "datetime_columns_gas = ['forecast_date']\n",
        "datetime_columns_electricity = ['forecast_date']\n",
        "datetime_columns_forecast = ['forecast_datetime']\n",
        "datetime_columns_historical = ['datetime']\n",
        "\n",
        "df_data = standardize_datetime_precision(df_data, datetime_columns_data)\n",
        "df_client = standardize_datetime_precision(df_client, datetime_columns_client)\n",
        "df_gas = standardize_datetime_precision(df_gas, datetime_columns_gas)\n",
        "df_electricity = standardize_datetime_precision(df_electricity, datetime_columns_electricity)\n",
        "df_forecast = standardize_datetime_precision(df_forecast, datetime_columns_forecast)\n",
        "df_historical = standardize_datetime_precision(df_historical, datetime_columns_historical)\n",
        "\n",
        "# Filtering data for years greater than 2021\n",
        "df_data = df_data.filter(pl.col('datetime').dt.year() > 2021)\n",
        "\n",
        "# Function to convert data into a pandas DataFrame\n",
        "def to_pandas(X, y=None):\n",
        "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
        "\n",
        "    if y is not None:\n",
        "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
        "    else:\n",
        "        df = X.to_pandas()\n",
        "\n",
        "    df = df.set_index(\"row_id\")\n",
        "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
        "\n",
        "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
        "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
        "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Define the feature engineering function\n",
        "def feature_eng(df):\n",
        "    # Example: Creating a new feature by combining existing ones\n",
        "    if 'installed_capacity' in df.columns and 'eic_count' in df.columns:\n",
        "        df = df.with_columns([(pl.col('installed_capacity') * pl.col('eic_count')).alias('capacity_eic_product')])\n",
        "\n",
        "    # Example: Transforming a feature (e.g., logarithmic transformation)\n",
        "    if 'euros_per_mwh' in df.columns:\n",
        "        df = df.with_columns([pl.col('euros_per_mwh').log().alias('log_euros_per_mwh')])\n",
        "\n",
        "    # Example: Encoding categorical variables\n",
        "    if 'county' in df.columns:\n",
        "        # Count the occurrences of each 'county'\n",
        "        county_counts = df.group_by('county').agg(pl.count().alias('county_frequency'))\n",
        "\n",
        "        # Join the counts back to the original dataframe\n",
        "        df = df.join(county_counts, on='county', how='left')\n",
        "\n",
        "    # Additional feature engineering steps...\n",
        "    # TO DO...\n",
        "\n",
        "    return df\n",
        "\n",
        "# Feature engineering and data preparation in batches\n",
        "number_of_batches = 100  # Adjust based on the memory capacity\n",
        "for batch in np.array_split(df_data.to_pandas(), number_of_batches):\n",
        "    X = pl.DataFrame(batch.drop(columns=[\"target\"]))\n",
        "    y = pl.DataFrame(batch[[\"target\"]])\n",
        "\n",
        "    # Apply feature engineering\n",
        "    X = feature_eng(X)\n",
        "\n",
        "# Garbage collection to free up memory\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T21:46:49.997652Z",
          "iopub.execute_input": "2023-12-14T21:46:49.998382Z",
          "iopub.status.idle": "2023-12-14T21:46:55.407019Z",
          "shell.execute_reply.started": "2023-12-14T21:46:49.998346Z",
          "shell.execute_reply": "2023-12-14T21:46:55.405860Z"
        },
        "trusted": true,
        "id": "og3yFcq5UOgg",
        "outputId": "958470dc-75d1-413a-e07b-e184e5f6adf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "faUdynxgUOgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Assuming X is a Polars DataFrame, convert it to a Pandas DataFrame\n",
        "X_df = X.to_pandas()\n",
        "\n",
        "# Ensure the 'datetime' column is in datetime format\n",
        "X_df['datetime'] = pd.to_datetime(X_df['datetime'])\n",
        "\n",
        "# Convert the 'datetime' column to a numeric format (e.g., seconds since epoch)\n",
        "X_df['datetime'] = X_df['datetime'].apply(lambda x: x.timestamp())\n",
        "\n",
        "# Data Preprocessing\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_df)\n",
        "\n",
        "# Splitting Data\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y.to_pandas(), test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert y_train and y_test to 1D array\n",
        "y_train = y_train.values.ravel()\n",
        "y_test = y_test.values.ravel()\n",
        "\n",
        "# Model Training\n",
        "# Initialize and train the model\n",
        "model = RandomForestRegressor(random_state=42)  # Use RandomForestClassifier for classification tasks\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the mean absolute error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"\\n-> Mean Absolute Error: {mae}\")\n",
        "\n",
        "# Hyperparameter Tuning (Optional)\n",
        "# This can be done using GridSearchCV or RandomizedSearchCV from sklearn.model_selection\n",
        "\n",
        "# Prediction\n",
        "# Use model.predict(new_data) to make predictions on new, unseen data\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T21:46:55.408393Z",
          "iopub.execute_input": "2023-12-14T21:46:55.408840Z",
          "iopub.status.idle": "2023-12-14T21:46:59.008600Z",
          "shell.execute_reply.started": "2023-12-14T21:46:55.408793Z",
          "shell.execute_reply": "2023-12-14T21:46:59.007406Z"
        },
        "trusted": true,
        "id": "LfeWuao9UOgi",
        "outputId": "d80dc47d-856c-430e-d171-5c41044b98e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n-> Mean Absolute Error: 59.44933381845689\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Initialize environment\n",
        "if 'env' not in globals():\n",
        "    env = enefit.make_env()\n",
        "\n",
        "# Initialize empty DataFrames\n",
        "df_forecast = pl.DataFrame({col: [] for col in forecast_cols})\n",
        "df_historical = pl.DataFrame({col: [] for col in historical_cols})\n",
        "df_target = pl.DataFrame({col: [] for col in target_cols})\n",
        "\n",
        "# Iterating over the test set provided by the environment\n",
        "iter_test = env.iter_test()\n",
        "\n",
        "for (test, revealed_targets, client, historical_weather, forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
        "    # Data type conversion and renaming\n",
        "    common_dtype = 'datetime64[ns]'\n",
        "    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
        "    test['datetime'] = test['datetime'].astype(common_dtype)\n",
        "    historical_weather['datetime'] = historical_weather['datetime'].astype(common_dtype)\n",
        "\n",
        "    # Correct timezone conversion for forecast_weather\n",
        "    if forecast_weather['forecast_datetime'].dt.tz is None:\n",
        "        forecast_weather['forecast_datetime'] = forecast_weather['forecast_datetime'].dt.tz_localize('UTC')\n",
        "    forecast_weather['forecast_datetime'] = forecast_weather['forecast_datetime'].dt.tz_convert(None)\n",
        "    forecast_weather['forecast_datetime'] = forecast_weather['forecast_datetime'].astype(common_dtype)\n",
        "\n",
        "    revealed_targets['datetime'] = revealed_targets['datetime'].astype(common_dtype)\n",
        "    client['date'] = client['date'].astype(common_dtype)\n",
        "\n",
        "    # Concatenating and making data unique\n",
        "    df_new_forecast = pl.from_pandas(forecast_weather[forecast_cols])\n",
        "    df_new_historical = pl.from_pandas(historical_weather[historical_cols])\n",
        "    df_new_target = pl.from_pandas(revealed_targets[target_cols])\n",
        "\n",
        "    df_forecast = df_forecast.cast(df_new_forecast.schema)\n",
        "    df_historical = df_historical.cast(df_new_historical.schema)\n",
        "    df_target = df_target.cast(df_new_target.schema)\n",
        "\n",
        "    df_forecast = pl.concat([df_forecast, df_new_forecast]).unique()\n",
        "    df_historical = pl.concat([df_historical, df_new_historical]).unique()\n",
        "    df_target = pl.concat([df_target, df_new_target]).unique()\n",
        "\n",
        "    # Convert test DataFrame to Polars DataFrame\n",
        "    test_pl = pl.from_pandas(test)\n",
        "\n",
        "    # Rename columns for joining, if they exist\n",
        "    if 'datetime_historical' in df_historical.columns:\n",
        "        df_historical = df_historical.rename({'datetime_historical': 'datetime'})\n",
        "    if 'datetime_target' in df_target.columns:\n",
        "        df_target = df_target.rename({'datetime_target': 'datetime'})\n",
        "    if 'county_target' in df_target.columns:\n",
        "        df_target = df_target.rename({'county_target': 'county'})\n",
        "\n",
        "    # Further processing and combining data for model input\n",
        "    # Joining DataFrames using Polars join function\n",
        "    X_test = test_pl.join(df_historical, on='datetime', how='left')\n",
        "    X_test = X_test.join(df_target, on=['datetime', 'county'], how='left')\n",
        "\n",
        "    # Convert to Pandas DataFrame for compatibility with scikit-learn\n",
        "    X_test_pandas = X_test.to_pandas()\n",
        "\n",
        "    # Using SimpleImputer to fill NaN values across all columns\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_test_imputed = imputer.fit_transform(X_test_pandas)\n",
        "\n",
        "    # Ensure only numeric columns are used for model prediction\n",
        "    X_test_for_model = pd.DataFrame(X_test_imputed, columns=X_test_pandas.columns).select_dtypes(include=[np.number])\n",
        "\n",
        "    # Check if the number of rows in X_test matches sample_prediction\n",
        "    if len(X_test_for_model) != len(sample_prediction):\n",
        "        print(f\"Warning: Mismatch in number of rows. X_test has {len(X_test_for_model)} rows, sample_prediction has {len(sample_prediction)} rows.\")\n",
        "\n",
        "    # Making predictions\n",
        "    predictions = model.predict(X_test_for_model)\n",
        "    if len(predictions) != len(sample_prediction):\n",
        "        print(f\"Warning: Mismatch in prediction length. Model returned {len(predictions)} predictions, expected {len(sample_prediction)}.\")\n",
        "\n",
        "    sample_prediction[\"target\"] = predictions.clip(0)\n",
        "\n",
        "    # Submitting predictions\n",
        "    env.predict(sample_prediction)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-14T21:46:59.010724Z",
          "iopub.execute_input": "2023-12-14T21:46:59.011345Z",
          "iopub.status.idle": "2023-12-14T21:47:02.511582Z",
          "shell.execute_reply.started": "2023-12-14T21:46:59.011273Z",
          "shell.execute_reply": "2023-12-14T21:47:02.509232Z"
        },
        "trusted": true,
        "id": "44znO6kmUOgi",
        "outputId": "d0220533-0831-4733-c9a2-a6efdc619d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Using SimpleImputer to fill NaN values across all columns\u001b[39;00m\n\u001b[1;32m     65\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m X_test_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_pandas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Ensure only numeric columns are used for model prediction\u001b[39;00m\n\u001b[1;32m     69\u001b[0m X_test_for_model \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test_imputed, columns\u001b[38;5;241m=\u001b[39mX_test_pandas\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py:390\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    382\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter was deprecated in version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1 and will be removed in 1.3. A warning will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    388\u001b[0m     )\n\u001b[0;32m--> 390\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/impute/_base.py:327\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    324\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:810\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m--> 810\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[1;32m    812\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:6324\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6317\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   6319\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m   6320\u001b[0m     ]\n\u001b[1;32m   6322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6323\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6324\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6327\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:451\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    449\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:511\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 511\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    515\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:242\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    239\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:184\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:701\u001b[0m, in \u001b[0;36mDatetimeArray.astype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_period_dtype(dtype):\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_period(freq\u001b[38;5;241m=\u001b[39mdtype\u001b[38;5;241m.\u001b[39mfreq)\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDatetimeLikeArrayMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:487\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.astype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    481\u001b[0m     is_datetime_or_timedelta_dtype(dtype)\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_equal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, dtype)\n\u001b[1;32m    483\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m is_float_dtype(dtype):\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# disallow conversion between datetime/timedelta,\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# and conversions for any datetimelike to float\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot cast \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 487\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot cast DatetimeArray to dtype float64"
          ],
          "ename": "TypeError",
          "evalue": "Cannot cast DatetimeArray to dtype float64",
          "output_type": "error"
        }
      ]
    }
  ]
}