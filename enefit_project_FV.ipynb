{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 57236,
          "databundleVersionId": 7230081,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-Energy-Comp/blob/main/enefit_project_FV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Description\n",
        "\n",
        "**Understand the specific problem of energy imbalance caused by prosumers and how the model can help Enefit**\n",
        "\n",
        "Specific Problem: Energy Imbalance Resulting from Prosumers\n",
        "\n",
        "The primary concern is the energy discrepancy that occurs when there is a disparity between the anticipated and the actual energy consumed or generated. The issue is worsened by prosumers contributing to the problem due to their simultaneous roles as energy consumers and producers. Their energy use and generation can be erratic, resulting in logistical and financial difficulties for energy firms like Enefit. These problems encompass the struggle to align supply and demand and the resulting expenditures from this imbalance.\n",
        "\n",
        "The Role of the Model in Facilitating Enefit\n",
        "The model aims to address these difficulties by offering precise forecasts of prosumers' energy usage and production. By doing so, the model will:\n",
        "\n",
        "1. Increase Forecasting Precision: Enhance Enefit's capacity to anticipate energy demands and production levels accurately.\n",
        "   \n",
        "2. Minimise Imbalance Expenses: Enefit can optimise its energy allocation by utilising more accurate forecasts, hence decreasing the expenses linked to energy imbalance.\n",
        "\n",
        "3. Enhance Resource Allocation Efficiency: Precise predictions will allow Enefit to distribute resources more optimally, thereby minimising waste and decreasing operational expenses.\n",
        "\n",
        "4. Enhance Strategic Decision-Making: Enefit can improve its ability to make strategic decisions on infrastructure investments and policy changes by gaining deeper insights into consumer behaviour.\n",
        "\n",
        "5. Encourage Sustainable Habits: Enefit may encourage prosumers to use renewable energy sources through efficient energy management, facilitating the shift towards more environmentally friendly energy habits.\n",
        "\n",
        "The model must incorporate multiple variables that impact consumer behaviour, such as weather patterns, past energy usage trends, pricing fluctuations, etc. The model's performance will be assessed based on its Mean Absolute Error (MAE), which requires your predictions to match the actual values to minimise the error measurement closely.\n",
        "\n",
        "The competition offers a dataset of historical meteorological data, energy pricing, and details regarding prosumer attributes. The given Python time-series API will guarantee that the model complies with the competition's specifications, including the prohibition of looking ahead in time and utilising just the available data for making predictions."
      ],
      "metadata": {
        "id": "iF04LaTg0ygt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup an Environment\n",
        "\n",
        "\n",
        "This code snippet is written in Python and includes a series of import statements, bringing various Python libraries and functions into the current script. These libraries are primarily used for data manipulation, machine learning, and optimization. Let us break down each part:\n",
        "\n",
        "1. **Import Statements**:\n",
        "   - `import os`: Imports the OS module, which provides functions for interacting with the operating system.\n",
        "   - `import gc`: Imports the garbage collector interface, which can manually trigger Python's garbage collection process.\n",
        "   - `import pickle`: Imports the pickle module used for serializing (pickling) and deserializing (unpickling) Python object structures.\n",
        "   - `import numpy as np`: Imports the NumPy library (as `np`), which is fundamental for scientific computing in Python, providing support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
        "   - `import pandas as pd`: Imports the pandas library (as `pd`), a powerful data manipulation and analysis tool.\n",
        "   - `import polars as pl`: Imports the Polars library (as `pl`), another data manipulation and analysis library, similar to pandas but often faster for certain operations.\n",
        "   - `from sklearn.model_selection import cross_val_score, cross_validate`: Imports `cross_val_score` and `cross_validate` functions from Scikit-Learn, used for cross-validation in machine learning.\n",
        "   - `from sklearn.metrics import mean_absolute_error`: Imports the Mean Absolute Error (MAE) metric from Scikit-Learn, used to evaluate the performance of regression models.\n",
        "   - `from sklearn.compose import TransformedTargetRegressor`: Imports a utility from Scikit-Learn to transform the target variable in regression problems.\n",
        "   - `from sklearn.ensemble import VotingRegressor`: Imports the VotingRegressor from Scikit-Learn, a meta-regressor that fits several base regressors and averages their predictions.\n",
        "   - `import lightgbm as lgb`: Imports LightGBM (as `lgb`), a gradient boosting framework that uses tree-based learning algorithms.\n",
        "   - `!pip install optuna`: Uses the pip package installer to install Optuna, a library for hyperparameter optimization.\n",
        "   - `import optuna`: Imports the Optuna library for optimizing machine learning models.\n",
        "\n",
        "2. **Purpose of the Code**:\n",
        "   - This code snippet creates an environment for data processing, machine learning, and hyperparameter optimization.\n",
        "   - It is likely part of a larger script or project focused on building, evaluating, and optimizing machine learning models, particularly regression models, given the import of `mean_absolute_error` and `TransformedTargetRegressor`.\n",
        "   - The use of `cross_val_score` and `cross_validate` suggests that cross-validation is used for model evaluation.\n",
        "   - `VotingRegressor` and `lightgbm` indicate that ensemble methods and gradient boosting are used in model training.\n",
        "   - `Optuna` is used to optimize the hyperparameters of these models to enhance performance.\n",
        "\n",
        "This code, by itself, doesn't perform any data analysis or machine learning operations. It is a setup for such tasks, defining the necessary libraries and tools to be used in subsequent code."
      ],
      "metadata": {
        "id": "UrdU4x2Lf_2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "!pip install optuna\n",
        "\n",
        "import optuna"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEWOCiWD9F6E",
        "outputId": "8e8bf325-aafc-4d59-8e9e-f4fcdb32b8e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.0 colorlog-6.8.0 optuna-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validation Strategy\n",
        "\n",
        "The provided code snippet defines a Python class named `MonthlyKFold`, which appears to be a custom implementation of a cross-validation strategy. This class is designed to use time series data where observations are organized by month and year. Let us break down the code:\n",
        "\n",
        "1. **Class Definition - `MonthlyKFold`**:\n",
        "   - The class `MonthlyKFold` is intended for cross-validation in machine learning, specifically for datasets with a time component (monthly data in this case).\n",
        "\n",
        "2. **Constructor - `__init__(self, n_splits=3)`**:\n",
        "   - The `__init__` method is the class's constructor. It initializes the instance with the number of splits (`n_splits`) for cross-validation. By default, the splits are set to 3 if not specified.\n",
        "\n",
        "3. **Method - `split(self, X, y, groups=None)`**:\n",
        "   - The `split` method is the main functionality of this class. It is called with the features (`X`) and the target (`y'), along with an optional `groups` parameter.\n",
        "   - The method calculates a unique identifier for each month by combining the year and month values from the dataset. This assumes that `X` has columns named \"year\" and \"month\".\n",
        "   - It then sorts these unique monthly timesteps.\n",
        "   - The method iterates over the last `n_splits` months. For each of these months:\n",
        "     - It determines the indices of the training set (data before the current month) and the test set (data of the current month).\n",
        "     - It yields these indices, allowing for splitting the dataset into training and testing sets for each fold in the cross-validation.\n",
        "\n",
        "4. **Method - `get_n_splits(self, X, y, groups=None)`**:\n",
        "   - This method returns the number of splits (`n_splits`) the cross-validation process will use. This is a standard method in Scikit-Learn's cross-validator classes.\n",
        "\n",
        "5. **Usage and Purpose**:\n",
        "   - This custom cross-validator is particularly useful for time-series data where you want to ensure that the validation set comes chronologically after the training set, respecting the temporal order.\n",
        "   - The design suggests it has been used for scenarios where models must be evaluated based on their performance on unseen future data, a common requirement in time-series forecasting.\n",
        "\n",
        "In summary, `MonthlyKFold` is a custom class for performing time-based cross-validation, particularly suited for monthly data sets. It ensures that the model is continually trained on past data and tested on future data, adhering to the chronological order, which is crucial in time-series analysis."
      ],
      "metadata": {
        "id": "O21a-By7kvT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MonthlyKFold:\n",
        "    def __init__(self, n_splits=3):\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split(self, X, y, groups=None):\n",
        "        dates = 12 * X[\"year\"] + X[\"month\"]\n",
        "        timesteps = sorted(dates.unique().tolist())\n",
        "        X = X.reset_index()\n",
        "\n",
        "        for t in timesteps[-self.n_splits:]:\n",
        "            idx_train = X[dates.values < t].index\n",
        "            idx_test = X[dates.values == t].index\n",
        "\n",
        "            yield idx_train, idx_test\n",
        "\n",
        "    def get_n_splits(self, X, y, groups=None):\n",
        "        return self.n_splits\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "iK0lfj249F6H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "The provided code snippet defines a Python function named `to_pandas`, which is designed to convert datasets into a Pandas DataFrame and perform specific transformations on it. Let us break down the function:\n",
        "\n",
        "1. **Function Definition - `to_pandas(X, y=None)`**:\n",
        "   - The function `to_pandas` takes two arguments: `X` and an optional `y'. `X` is presumably a dataset, and `y' could be a target variable associated with `X`.\n",
        "   - The function is designed to work with datasets that have methods similar to the Pandas DataFrame (like `.to_pandas()`), suggesting that `X` and `y' might be in a format used by libraries like Polars, or they could be Pandas DataFrames already.\n",
        "\n",
        "2. **Converting to Pandas DataFrame**:\n",
        "   - If `y' is provided, the function concatenates `X` and `y' along the columns (axis=1) after converting them to Pandas DataFrames. This suggests that `X` and `y' are separate but related datasets, like features and target labels in machine learning.\n",
        "   - If `y' is not provided, it converts `X` to a Pandas DataFrame.\n",
        "\n",
        "3. **Data Transformation**:\n",
        "   - The function defines a list of categorical columns (`cat_cols`). It then sets the \"row_id\" column as the index of the DataFrame and converts the columns in `cat_cols` to the categorical data type. This is often done to optimize memory usage and improve performance in certain operations.\n",
        "   - It calculates the mean and standard deviation of a series of columns named \"target_1\" through \"target_6\" and creates new columns \"target_mean\" and \"target_std\" to store these values. This is typically done for feature engineering, where new features are derived from existing data.\n",
        "   - It also calculates a ratio of \"target_6\" to \"target_7\" (with a small constant added to the denominator to avoid division by zero) and stores this in a new column \"target_ratio\".\n",
        "\n",
        "4. **Return Value**:\n",
        "   - The function returns the transformed DataFrame.\n",
        "\n",
        "5. **Usage and Purpose**:\n",
        "   - This function is likely used in a data processing pipeline, especially in contexts where machine learning models are being developed.\n",
        "   - Its primary purpose is to prepare the data by converting it into a format suitable for analysis (Pandas DataFrame), setting the correct index, converting specific columns to categorical types for better processing, and creating new features valuable for predictive modelling.\n",
        "\n",
        "In summary, `to_pandas` is a utility function for data preparation, particularly in a machine-learning context. It transforms datasets by converting them into Pandas DataFrames, setting appropriate data types, and engineering new features."
      ],
      "metadata": {
        "id": "WwVpMVpHsRnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_pandas(X, y=None):\n",
        "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
        "\n",
        "    if y is not None:\n",
        "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
        "    else:\n",
        "        df = X.to_pandas()\n",
        "\n",
        "    df = df.set_index(\"row_id\")\n",
        "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
        "\n",
        "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
        "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
        "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "pkec2e4y9F6I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Optimization\n",
        "\n",
        "The provided code snippet defines a Python function named `lgb_objective`, which appears to be designed as an objective function for hyperparameter optimization using Optuna, specifically for a LightGBM regression model. Let us break down the function:\n",
        "\n",
        "1. **Function Definition - `lgb_objective(trial)`**:\n",
        "   - The function `lgb_objective` takes a single argument `trial`, representing an Optuna trial. An Optuna trial is used to suggest hyperparameters and evaluate their performance.\n",
        "\n",
        "2. **Hyperparameter Suggestions**:\n",
        "   - Inside the function, a dictionary named `params` holds the hyperparameters for a LightGBM model.\n",
        "   - The function uses `trial.suggest_*` methods to define a range of values for various hyperparameters. These methods are part of Optuna and are used to sample hyperparameters from specified ranges:\n",
        "     - `learning_rate`: A float between 0.01 and 0.1.\n",
        "     - `colsample_bytree` and `colsample_bynode`: Floats between 0.5 and 1.0, specifying the subsample ratio of columns when constructing each tree/node.\n",
        "     - `lambda_l1` and `lambda_l2`: Regularization parameters float between 0.01 and 10.0.\n",
        "     - `min_data_in_leaf`: An integer between 4 and 256, specifying the minimum number of samples per leaf node.\n",
        "     - `max_depth`: An integer between 5 and 10, specifying the maximum depth of the trees.\n",
        "     - `max_bin`: An integer between 32 and 1024, specifying the maximum number of bins used for constructing histograms.\n",
        "   - Fixed parameters like `n_iter`, `verbose`, `random_state`, and `objective` are also set.\n",
        "\n",
        "3. **Model Training and Cross-Validation**:\n",
        "   - A LightGBM regressor model is instantiated with the suggested parameters.\n",
        "   - The function assumes the existence of a DataFrame `df_train`, from which it separates the features (`X`) and the target variable (`y').\n",
        "   - It uses a previously defined `MonthlyKFold` object for cross-validation. This is likely the same `MonthlyKFold` class defined in a previous code snippet, which handles time-based splitting for cross-validation.\n",
        "   - The model's performance is evaluated using the `cross_val_score` function from Scikit-Learn, with the scoring metric set to 'neg_mean_absolute_error'.\n",
        "\n",
        "4. **Objective Value Return**:\n",
        "   - The function returns the negative mean of the cross-validation scores. In optimization, we often minimize a value, so returning the negative mean absolute error makes it a minimization problem. Optuna will aim to find hyperparameters that minimize this returned value.\n",
        "\n",
        "5. **Usage and Purpose**:\n",
        "   - This function is used in hyperparameter tuning using Optuna for a LightGBM regression model.\n",
        "   - It is designed to iteratively test different sets of hyperparameters, evaluate their performance using cross-validation, and return a score that reflects the average performance of a model with those hyperparameters.\n",
        "   - The Optuna framework uses this function to identify the best set of hyperparameters for the model based on the objective function's output.\n",
        "\n",
        "In summary, `lgb_objective` is an objective function for hyperparameter optimization of a LightGBM regression model, utilizing cross-validation and designed to be used with the Optuna hyperparameter optimization framework."
      ],
      "metadata": {
        "id": "wvmCA5FHvf2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_objective(trial):\n",
        "    params = {\n",
        "        'n_iter'           : 1000,\n",
        "        'verbose'          : -1,\n",
        "        'random_state'     : 42,\n",
        "        'objective'        : 'l2',\n",
        "        'learning_rate'    : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'colsample_bynode' : trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
        "        'lambda_l1'        : trial.suggest_float('lambda_l1', 1e-2, 10.0),\n",
        "        'lambda_l2'        : trial.suggest_float('lambda_l2', 1e-2, 10.0),\n",
        "        'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 4, 256),\n",
        "        'max_depth'        : trial.suggest_int('max_depth', 5, 10),\n",
        "        'max_bin'          : trial.suggest_int('max_bin', 32, 1024),\n",
        "    }\n",
        "\n",
        "    model  = lgb.LGBMRegressor(**params)\n",
        "    X, y   = df_train.drop(columns=[\"target\"]), df_train[\"target\"]\n",
        "    cv     = MonthlyKFold(1)\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
        "\n",
        "    return -1 * np.mean(scores)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UpjmRMGP9F6J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "This Python code defines several functions focused on data preprocessing, particularly for handling time series and geospatial data using the Polars library. It also includes a comprehensive function for feature engineering that integrates all these preprocessing steps. Let us break down each function and the overall workflow:\n",
        "\n",
        "1. **`convert_to_polars(*dfs)`**:\n",
        "   - Converts a list of data frames (possibly in different formats) to Polaris DataFrames. It checks if each dataframe is already a Polars DataFrame; if not, it converts it.\n",
        "\n",
        "2. **`process_datetime(df, column_name, is_date=False)`**:\n",
        "   - Processes datetime columns in a dataframe to ensure they are in the correct format (either `pl.Date` or `pl.Datetime`). If `is_date` is `True, ' it processes the column as a date; otherwise, it processes it as a datetime.\n",
        "\n",
        "3. **`process_location(df)`**:\n",
        "   - Converts latitude and longitude columns in a dataframe to `float32` type. This is useful for geospatial data processing.\n",
        "\n",
        "4. **`join_dataframes(df_main, dfs, join_conditions, suffixes)`**:\n",
        "   - Joins multiple data frames with specified conditions and suffixes. It iterates through a list of data frames (`dfs`), joining each to the main data frame (`df_main`) based on the join conditions. If the specified join condition columns are not present in the main and the joining data frame, the join is skipped for that data frame.\n",
        "\n",
        "5. **`add_time_features(df)`**:\n",
        "   - Adds time-related features to a dataframe, such as ordinal day, hour, day, weekday, month, year, and trigonometric transformations of day and hour. This is useful for capturing cyclical nature in time data.\n",
        "\n",
        "6. **`feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)`**:\n",
        "   - This is the primary feature engineering function. It takes multiple data frames as inputs, each representing different aspects of a dataset (like client information, gas usage, electricity usage, forecasts, historical data, location data, and target variables).\n",
        "   - Each dataframe is processed using the previously defined functions. Datetime columns are formatted correctly, location data is cast to float, and the data frames are joined based on specified conditions.\n",
        "   - Time-related features are added to the main dataframe.\n",
        "   - Unnecessary columns are optionally dropped at the end.\n",
        "   - The processed and feature-engineered main dataframe is returned.\n",
        "\n",
        "This code is a comprehensive approach to preprocessing and feature engineering for a dataset that combines multiple data sources, including time series and geospatial data. It is likely part of a more extensive data analysis or machine learning pipeline where such preprocessing is crucial for model training and analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "7MAmk-OPxcWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "\n",
        "def convert_to_polars(*dfs):\n",
        "    \"\"\"Converts a list of dataframes to Polars DataFrames.\"\"\"\n",
        "    return [pl.DataFrame(df) if not isinstance(df, pl.DataFrame) else df for df in dfs]\n",
        "\n",
        "def process_datetime(df, column_name, is_date=False):\n",
        "    \"\"\"Processes datetime columns to ensure correct format.\"\"\"\n",
        "    if column_name in df.columns:\n",
        "        if is_date and df.dtypes[df.columns.index(column_name)] == pl.Date:\n",
        "            df = df.with_columns(pl.col(column_name).cast(pl.Date))\n",
        "        elif not is_date and df.dtypes[df.columns.index(column_name)] == pl.Datetime:\n",
        "            df = df.with_columns(pl.col(column_name).cast(pl.Datetime))\n",
        "    return df\n",
        "\n",
        "def process_location(df):\n",
        "    \"\"\"Converts latitude and longitude to float.\"\"\"\n",
        "    return df.with_columns(\n",
        "        pl.col(\"latitude\").cast(pl.Float32),\n",
        "        pl.col(\"longitude\").cast(pl.Float32)\n",
        "    )\n",
        "\n",
        "def join_dataframes(df_main, dfs, join_conditions, suffixes):\n",
        "    \"\"\"Joins multiple dataframes with specified conditions and suffixes.\"\"\"\n",
        "    for df, condition, suffix in zip(dfs, join_conditions, suffixes):\n",
        "        if isinstance(condition, list):\n",
        "            condition_check = all(col in df_main.columns and col in df.columns for col in condition)\n",
        "        else:\n",
        "            condition_check = condition in df_main.columns and condition in df.columns\n",
        "\n",
        "        if condition_check:\n",
        "            df_main = df_main.join(df, on=condition, how=\"left\", suffix=suffix)\n",
        "        else:\n",
        "            print(f\"Skipping join for {suffix} due to missing column in condition: {condition}\")\n",
        "    return df_main\n",
        "\n",
        "def add_time_features(df):\n",
        "    \"\"\"Adds time-related features to the dataframe.\"\"\"\n",
        "    if \"datetime\" in df.columns and df.dtypes[df.columns.index(\"datetime\")] in [pl.Datetime, pl.Date]:\n",
        "        df = df.with_columns(\n",
        "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
        "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
        "            pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
        "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
        "            pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
        "            pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
        "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
        "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
        "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
        "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\")\n",
        "        )\n",
        "    else:\n",
        "        print(\"Warning: 'datetime' column not found. Time features cannot be added.\")\n",
        "    return df\n",
        "\n",
        "def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target):\n",
        "    # Convert to Polars DataFrame\n",
        "    df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target = convert_to_polars(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "\n",
        "    # Process each DataFrame\n",
        "    df_data = process_datetime(df_data, \"datetime\")\n",
        "    df_client = process_datetime(df_client, \"date\", is_date=True)\n",
        "    df_gas = process_datetime(df_gas, \"forecast_date\", is_date=True)\n",
        "    df_electricity = process_datetime(df_electricity, \"forecast_date\")\n",
        "    df_location = process_location(df_location)\n",
        "    df_forecast = process_datetime(df_forecast, \"forecast_datetime\")\n",
        "    df_historical = process_datetime(df_historical, \"datetime\")  # Assuming df_historical has a datetime column\n",
        "\n",
        "    # Define join conditions and suffixes\n",
        "    join_conditions = [\n",
        "        \"date\",\n",
        "        [\"county\", \"is_business\", \"product_type\", \"date\"],\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        [\"county\", \"datetime\"],\n",
        "        \"datetime\"\n",
        "    ]\n",
        "    suffixes = [\"_gas\", \"_client\", \"_elec\", \"_fcast\", \"_hist\", \"_loc\", \"_target\"]\n",
        "\n",
        "    # Join dataframes\n",
        "    df_data = join_dataframes(df_data, [df_gas, df_client, df_electricity, df_forecast, df_historical, df_location, df_target], join_conditions, suffixes)\n",
        "\n",
        "    # Add time-related features\n",
        "    df_data = add_time_features(df_data)\n",
        "\n",
        "    # Optionally, drop unnecessary columns\n",
        "    df_data = df_data.drop([\"date\", \"datetime\", \"hour\", \"dayofyear\"])\n",
        "\n",
        "    return df_data\n"
      ],
      "metadata": {
        "id": "UYofNURu_2qc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables\n",
        "\n",
        "The code snippet provided is a data preprocessing setup in a Python script, likely for a data analysis or machine learning project focused on energy behaviour prediction. It defines variables that specify file paths, column names for different datasets, and paths for saving or loading processed data. Let us break it down:\n",
        "\n",
        "1. **File Paths:**\n",
        "   - `root`: Defines the root directory where the data files are stored. It is currently set to a path in Google Drive, indicating that this script might be used in a Google Colab environment. The commented-out path suggests an alternative location, possibly for a Kaggle kernel.\n",
        "\n",
        "2. **Column Name Lists:**\n",
        "   - These lists define specific columns to be used or expected in different datasets. Each list corresponds to a different aspect of the data:\n",
        "     - `data_cols`: The main data columns, likely the primary dataset.\n",
        "     - `client_cols`: Columns related to client information.\n",
        "     - `gas_cols`: Columns related to gas price forecasts.\n",
        "     - `electricity_cols`: Columns related to electricity price forecasts.\n",
        "     - `forecast_cols`: Columns related to weather forecasts.\n",
        "     - `historical_cols`: Columns related to historical weather data.\n",
        "     - `location_cols`: Columns related to geographical location.\n",
        "     - `target_cols`: Columns related to target variables for prediction.\n",
        "\n",
        "3. **Save and Load Paths:**\n",
        "   - `save_path` and `load_path` are placeholders for paths where processed data can be saved or loaded. They are currently set to `None`, indicating that they will either be defined later in the script or that it currently does not use these functionalities.\n",
        "\n",
        "4. **Usage and Purpose:**\n",
        "   - This setup is typically used in data processing scripts where different datasets (like client data, energy forecasts, weather forecasts, historical weather data, etc.) are read, processed, and possibly joined or merged for analysis.\n",
        "   - The defined column lists help select, filter, or process specific parts of these datasets.\n",
        "   - The root path and the save/load paths are essential for file management in data processing workflows, especially when working with large datasets or in cloud environments like Google Colab.\n",
        "\n",
        "In summary, this code snippet is a preparatory part of a larger data processing script, setting up necessary variables for file paths and dataset columns, which will be used in subsequent data loading, preprocessing, and analysis steps in a project related to predicting energy behaviour."
      ],
      "metadata": {
        "id": "Gn9cdSMD9F6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
        "root = \"/content/drive/MyDrive/project_energy\"\n",
        "\n",
        "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
        "location_cols    = ['longitude', 'latitude', 'county']\n",
        "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
        "\n",
        "save_path = None\n",
        "load_path = None\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "zx4JfvPl9F6K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "3NWf6Ol49F6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports the 'drive' module from 'google.colab' and mounts the Google Drive to\n",
        "# the '/content/drive' directory in the Colab environment.\n",
        "from google.colab import drive\n",
        "\n",
        "# This function mounts Google Drive\n",
        "def mount_google_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function to mount Google Drive\n",
        "mount_google_drive()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5KJZJ3J-QNR",
        "outputId": "fda77f8d-e18b-49b4-8c24-9a662e90c39d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data\n",
        "\n",
        "This code snippet is part of a data loading and schema inspection process in a Python script, using the Polars library to handle CSV files. It reads multiple datasets from CSV files, selects specific columns, and extracts their schemas. Let us go through each part of the code:\n",
        "\n",
        "1. **Reading CSV Files with Polars**:\n",
        "   - The code uses `pl.read_csv()` from the Polars library to read various CSV files. Each file represents a different aspect of the data related to energy behaviour prediction.\n",
        "   - `os.path.join(root, \"filename.csv\")` constructs the file path for each CSV file, using the `root` variable defined earlier as the base directory.\n",
        "   The `columns` parameter in `pl.read_csv()` specifies which columns to read from each CSV file. The relevant column names are provided by the variables defined in the previous code snippet (like `data_cols`, `client_cols`, etc.).\n",
        "   - `try_parse_dates=True` attempts to automatically parse columns recognized as date columns into appropriate date formats.\n",
        "\n",
        "2. **DataFrames for Different Data Aspects**:\n",
        "   - `df_data`, `df_client`, `df_gas`, `df_electricity`, `df_forecast`, `df_historical`, and `df_location` are the DataFrames created for the training data, client information, gas prices, electricity prices, weather forecast, historical weather, and location mapping, respectively.\n",
        "   - `df_target` is created by selecting target columns from `df_data` using the `select()` method.\n",
        "\n",
        "3. **Schema Inspection**:\n",
        "   - For each DataFrame, the code extracts its schema (data types of each column) and stores it in a corresponding schema variable (like `schema_data`, `schema_client`, etc.).\n",
        "   - The schema of a DataFrame provides information about the type of data in each column, which is crucial for data preprocessing and understanding the nature of the data.\n",
        "\n",
        "4. **Purpose and Usage**:\n",
        "   - This code is used for loading and initial inspection of various datasets that will likely be used in an energy behaviour prediction project.\n",
        "   - It ensures that the data is loaded with the correct columns and provides an initial look at the data types, essential for subsequent data processing and analysis steps.\n",
        "   - The separation of data into different frames based on their nature (like client data, gas prices, etc.) suggests a structured approach to handling a complex dataset with multiple facets.\n",
        "\n",
        "In summary, the code is part of a data loading and exploration process, focusing on reading different datasets related to energy behaviour, selecting specific columns, parsing date columns where possible, and inspecting the data types of each column for further processing and analysis."
      ],
      "metadata": {
        "id": "YBthTl6M2exN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data        = pl.read_csv(os.path.join(root, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
        "df_client      = pl.read_csv(os.path.join(root, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
        "df_gas         = pl.read_csv(os.path.join(root, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
        "df_electricity = pl.read_csv(os.path.join(root, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
        "df_forecast    = pl.read_csv(os.path.join(root, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
        "df_historical  = pl.read_csv(os.path.join(root, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
        "df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
        "df_target      = df_data.select(target_cols)\n",
        "\n",
        "schema_data        = df_data.schema\n",
        "schema_client      = df_client.schema\n",
        "schema_gas         = df_gas.schema\n",
        "schema_electricity = df_electricity.schema\n",
        "schema_forecast    = df_forecast.schema\n",
        "schema_historical  = df_historical.schema\n",
        "schema_target      = df_target.schema\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ntnXGXI39F6L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Data for ML\n",
        "\n",
        "This Python code is designed to process and analyze a large dataset related to the energy behaviour of \"prosumers\" (entities that produce and consume energy). It involves several steps, from reading and preprocessing data to feature engineering. Let us break down the key components and their functions:\n",
        "\n",
        "1. **Import Libraries and Define Paths:**\n",
        "   - `os`, `gc`, `numpy`, `polars`: Import necessary libraries for file path handling, memory management, numerical operations, and data manipulation.\n",
        "   - `root`: A variable storing the root directory path where the data files are located.\n",
        "\n",
        "2. **Column Specifications:**\n",
        "   - Various lists (`data_cols`, `client_cols`, `gas_cols`, etc.) are defined, specifying the columns used from different CSV files.\n",
        "\n",
        "3. **Helper Function - `read_csv_polars`:**\n",
        "   - A function to read large CSV files efficiently into Polars DataFrames, which are similar to Pandas DataFrames but optimized for performance.\n",
        "\n",
        "4. **Reading CSV Files:**\n",
        "   - Utilizing the `read_csv_polars` function to read various CSV files into Polars DataFrames, such as `df_data`, `df_client`, `df_gas`, and others.\n",
        "\n",
        "5. **Date Format and Conversion:**\n",
        "   - `date_format`: A string specifying the format of datetime columns in the data.\n",
        "   - Several DataFrame columns are converted to datetime format using Polars' `str.strptime` method.\n",
        "\n",
        "6. **Standardize Datetime Precision:**\n",
        "   - A function `standardize_datetime_precision` is defined to ensure that all datetime columns across DataFrames have the same precision level.\n",
        "\n",
        "7. **Filtering Data:**\n",
        "   - `df_data` is filtered only to include records where the year in the 'datetime' column is greater than 2021.\n",
        "\n",
        "8. **Feature Engineering and Data Preparation:**\n",
        "   - The data is split into batches to manage memory usage efficiently.\n",
        "   - Each batch undergoes a process of dropping the target column, performing feature engineering (not explicitly defined in the provided code), and preparing the features (`X`) and target (`y') for further analysis or modelling.\n",
        "\n",
        "9. **Garbage Collection:**\n",
        "   - `gc.collect()` is called to free up memory by cleaning unused objects.\n",
        "\n",
        "This code is tailored for handling large datasets memory-efficiently, specifically focusing on energy data involving prices, consumption, and weather forecasts. The feature engineering part is implied but needs to be explicitly defined, suggesting that this is part of a more extensive data processing or machine learning workflow."
      ],
      "metadata": {
        "id": "feLGWmSh9F6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import polars as pl  # Polars is used for data manipulation\n",
        "\n",
        "# root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
        "\n",
        "# Columns specifications\n",
        "data_cols = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high', 'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation', 'diffuse_radiation', 'latitude', 'longitude']\n",
        "location_cols = ['longitude', 'latitude', 'county']\n",
        "\n",
        "# Helper function to read large CSV files directly into Polars DataFrames\n",
        "def read_csv_polars(file_path, columns, batch_size=5 * 10 ** 5):  # Adjust batch_size as needed\n",
        "    return pl.read_csv(file_path, columns=columns, batch_size=batch_size)\n",
        "\n",
        "# Reading CSV files directly into Polars DataFrames\n",
        "df_data = read_csv_polars(os.path.join(root, \"train.csv\"), data_cols)\n",
        "df_client = read_csv_polars(os.path.join(root, \"client.csv\"), client_cols)\n",
        "df_gas = read_csv_polars(os.path.join(root, \"gas_prices.csv\"), gas_cols)\n",
        "df_electricity = read_csv_polars(os.path.join(root, \"electricity_prices.csv\"), electricity_cols)\n",
        "df_forecast = read_csv_polars(os.path.join(root, \"forecast_weather.csv\"), forecast_cols)\n",
        "df_historical = read_csv_polars(os.path.join(root, \"historical_weather.csv\"), historical_cols)\n",
        "df_location = read_csv_polars(os.path.join(root, \"weather_station_to_county_mapping.csv\"), location_cols)\n",
        "\n",
        "# Define a date format string compatible with your data\n",
        "date_format = \"%Y-%m-%d %H:%M:%S\"  # Adjust the format according to your data\n",
        "\n",
        "# Convert 'datetime' columns to datetime format in all dataframes\n",
        "df_data = df_data.with_columns([pl.col('datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_client = df_client.with_columns([pl.col('date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_gas = df_gas.with_columns([pl.col('forecast_date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_electricity = df_electricity.with_columns([pl.col('forecast_date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_forecast = df_forecast.with_columns([pl.col('forecast_datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_historical = df_historical.with_columns([pl.col('datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "\n",
        "# Standardize datetime precision to microseconds\n",
        "# This function is simplified assuming your datetime data is already timezone-naive\n",
        "def standardize_datetime_precision(df, datetime_columns):\n",
        "    for col in datetime_columns:\n",
        "        df = df.with_columns([pl.col(col).cast(pl.Datetime)])\n",
        "    return df\n",
        "\n",
        "datetime_columns_data = ['datetime']\n",
        "datetime_columns_client = ['date']\n",
        "datetime_columns_gas = ['forecast_date']\n",
        "datetime_columns_electricity = ['forecast_date']\n",
        "datetime_columns_forecast = ['forecast_datetime']\n",
        "datetime_columns_historical = ['datetime']\n",
        "\n",
        "df_data = standardize_datetime_precision(df_data, datetime_columns_data)\n",
        "df_client = standardize_datetime_precision(df_client, datetime_columns_client)\n",
        "df_gas = standardize_datetime_precision(df_gas, datetime_columns_gas)\n",
        "df_electricity = standardize_datetime_precision(df_electricity, datetime_columns_electricity)\n",
        "df_forecast = standardize_datetime_precision(df_forecast, datetime_columns_forecast)\n",
        "df_historical = standardize_datetime_precision(df_historical, datetime_columns_historical)\n",
        "\n",
        "# Filtering data for years greater than 2021\n",
        "df_data = df_data.filter(pl.col('datetime').dt.year() > 2021)\n",
        "\n",
        "# Feature engineering and data preparation in batches\n",
        "# Note: Ensure the feature_eng function is defined and available in your notebook.\n",
        "number_of_batches = 100  # Adjust based on your memory capacity\n",
        "for batch in np.array_split(df_data.to_pandas(), number_of_batches):\n",
        "    X = pl.DataFrame(batch.drop(columns=[\"target\"]))\n",
        "    y = pl.DataFrame(batch[[\"target\"]])\n",
        "    # Here, you should include the feature engineering logic\n",
        "    # X = feature_eng(X, ...)\n",
        "\n",
        "    # Insert your code here to process X and y\n",
        "\n",
        "# Garbage collection to free up memory\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "mk3CrqpS9F6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f309b42-634f-43a7-ff41-00363afd9dc7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "The provided code snippet defines a Python dictionary named `best_params`, which appears to contain a set of optimized hyperparameters for a machine learning model, specifically a LightGBM model, given the nature of the parameters. Let us go through each key-value pair in the dictionary:\n",
        "\n",
        "1. **`'n_iter': 900`**:\n",
        "   - Specifies the number of iterations for the model training process. In the context of LightGBM, this typically refers to the number of boosting rounds.\n",
        "\n",
        "2. **`'verbose': -1`**:\n",
        "   - Sets the verbosity level for the model's training process. A value of `-1` generally means that the process will be silent, i.e., no logs will be shown during training.\n",
        "\n",
        "3. **`'objective': 'l2'`**:\n",
        "   - Indicates the objective function to be used by the model. Here, `'l2'` refers to the L2 loss, also known as mean squared error, commonly used for regression tasks.\n",
        "\n",
        "4. **`'learning_rate': 0.05689066836106983`**:\n",
        "   - This is the learning rate of the model, a crucial hyperparameter in gradient-boosting models. It determines the step size at each iteration while moving towards a minimum of the loss function.\n",
        "\n",
        "5. **`'colsample_bytree': 0.8915976762048253`**:\n",
        "   - Specifies the subsample ratio of columns when constructing each tree. Values closer to 1 mean more columns are used to build each tree.\n",
        "\n",
        "6. **`'colsample_bynode': 0.5942203285139224`**:\n",
        "   - This parameter is similar to `colsample_bytree` but applies to each node of the trees, specifying the subsample ratio of columns for each split.\n",
        "\n",
        "7. **`'lambda_l1': 3.6277555139102864`** and **`'lambda_l2': 1.6591278779517808`**:\n",
        "   - These represent L1 (Lasso) and L2 (Ridge) regularization terms. They are used to prevent overfitting by adding penalties to the model.\n",
        "\n",
        "8. **`'min_data_in_leaf': 186`**:\n",
        "   - Defines the minimum number of data points required to form a leaf. This can be used to control overfitting.\n",
        "\n",
        "9. **`'max_depth': 9`**:\n",
        "   - Specifies the maximum depth of each tree. Deeper trees can model more complex patterns but can also lead to overfitting.\n",
        "\n",
        "10. **`'max_bin': 813`**:\n",
        "    - Determines the maximum number of bins used for bucketing feature values. Higher numbers allow the algorithm to consider more split points, potentially leading to more accurate models but increasing computation.\n",
        "\n",
        "The dictionary `best_params` suggests that these parameters were likely obtained through a hyperparameter tuning process, possibly using a tool like Optuna, as indicated in previous code snippets. These optimized parameters are usually used to configure a LightGBM model to achieve better performance on a specific dataset. The exact values are tailored to the data's characteristics and the machine-learning task's specific requirements."
      ],
      "metadata": {
        "id": "pgM4VhGW9F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {\n",
        "    'n_iter'           : 900,\n",
        "    'verbose'          : -1,\n",
        "    'objective'        : 'l2',\n",
        "    'learning_rate'    : 0.05689066836106983,\n",
        "    'colsample_bytree' : 0.8915976762048253,\n",
        "    'colsample_bynode' : 0.5942203285139224,\n",
        "    'lambda_l1'        : 3.6277555139102864,\n",
        "    'lambda_l2'        : 1.6591278779517808,\n",
        "    'min_data_in_leaf' : 186,\n",
        "    'max_depth'        : 9,\n",
        "    'max_bin'          : 813,\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "UzMaXRoU9F6N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "\n",
        "This code snippet appears to be part of a machine learning pipeline using Python, involving data preprocessing, model training, and model serialization. Let us break down its functionality:\n",
        "\n",
        "1. **Import Libraries**:\n",
        "    - `pickle`: Used for serializing and deserializing Python object structures.\n",
        "    - `lightgbm as lgb`: Imports LightGBM, a gradient boosting framework for machine learning.\n",
        "    - `from sklearn.impute import SimpleImputer`: Imports `SimpleImputer` from scikit-learn, used for handling missing data.\n",
        "    - `import polars as pl`: Imports Polars, a fast DataFrame library.\n",
        "\n",
        "2. **Define File Paths**:\n",
        "    - `load_path = None`: Placeholder for a path to load a pre-trained model. Currently set to `None`.\n",
        "    - `save_path = \"path_to_save_model.pkl\"`: Path where the trained model will be saved.\n",
        "\n",
        "3. **Data Preprocessing**:\n",
        "    - Creates a `SimpleImputer` object with a specified strategy ('mean' or 'median') to handle missing values.\n",
        "    - Applies the imputer to the \"target\" column of a Polars DataFrame `df_data`.\n",
        "    - Converts the imputed data (a NumPy array) back into a Polars Series and renames it to \"target\".\n",
        "    - Updates the 'target' column in `df_data` with this imputed data.\n",
        "\n",
        "4. **Model Loading/Training**:\n",
        "    - Checks if `load_path` is not `None`. If a path is provided, it loads a pre-trained model using `pickle`.\n",
        "    - If `load_path` is `None`, it initializes a `VotingRegressor` model with multiple LightGBM regressors, each with different random states and presumably the same hyperparameters (`best_params`).\n",
        "    - Trains the model using the features (all columns except \"target\") and the target variable from `df_data`.\n",
        "\n",
        "5. **Model Saving**:\n",
        "    - If `save_path` is not `None`, serialize the trained model to the specified file using `pickle`.\n",
        "\n",
        "The code assumes that certain variables like `df_data` and `best_params` are already defined in the context where this script is run. Additionally, the actual construction and usage of a `VotingRegressor` might require further imports and adjustments, as the standard `VotingRegressor` in sci-kit-learn does not directly support LightGBM regressors without wrapping them in a sci-kit-learn compatible way."
      ],
      "metadata": {
        "id": "BMRSXOv39F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import lightgbm as lgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "import polars as pl\n",
        "\n",
        "load_path = None\n",
        "save_path = \"path_to_save_model.pkl\"\n",
        "\n",
        "# Assuming df_data is a Polars DataFrame\n",
        "imputer = SimpleImputer(strategy='mean')  # or strategy='median'\n",
        "imputed_data = imputer.fit_transform(df_data[[\"target\"]])\n",
        "\n",
        "# Convert the NumPy array to a Polars Series and rename it\n",
        "imputed_series = pl.Series(\"target\", imputed_data.flatten())\n",
        "\n",
        "# Update the 'target' column with imputed data\n",
        "df_data = df_data.with_columns([imputed_series])\n",
        "\n",
        "if load_path is not None:\n",
        "    model = pickle.load(open(load_path, \"rb\"))\n",
        "else:\n",
        "    # Assuming best_params is defined somewhere in your code\n",
        "    model = VotingRegressor([\n",
        "        ('lgb_1', lgb.LGBMRegressor(**best_params, random_state=100)),\n",
        "        ('lgb_2', lgb.LGBMRegressor(**best_params, random_state=101)),\n",
        "        ('lgb_3', lgb.LGBMRegressor(**best_params, random_state=102)),\n",
        "        ('lgb_4', lgb.LGBMRegressor(**best_params, random_state=103)),\n",
        "        ('lgb_5', lgb.LGBMRegressor(**best_params, random_state=104)),\n",
        "    ])\n",
        "    model.fit(\n",
        "        X=df_data.drop(columns=[\"target\"]),\n",
        "        y=df_data[\"target\"])\n",
        "\n",
        "if save_path is not None:\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "iKaH3Phl9F6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa73a0d-1f2f-4cc1-bbd0-2f391eab760d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local Training and Prediction Environment"
      ],
      "metadata": {
        "id": "Kg6pjYc2dqds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import lightgbm as lgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "import polars as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "load_path = None\n",
        "save_path = \"path_to_save_model.pkl\"\n",
        "\n",
        "# Assuming df_data is a Polars DataFrame\n",
        "imputer = SimpleImputer(strategy='mean')  # or strategy='median'\n",
        "imputed_data = imputer.fit_transform(df_data[[\"target\"]])\n",
        "\n",
        "# Convert the NumPy array to a Polars Series and rename it\n",
        "imputed_series = pl.Series(\"target\", imputed_data.flatten())\n",
        "\n",
        "# Update the 'target' column with imputed data\n",
        "df_data = df_data.with_columns([imputed_series])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_data.drop(columns=[\"target\"]), df_data[\"target\"], test_size=0.2, random_state=42)\n",
        "\n",
        "if load_path is not None:\n",
        "    model = pickle.load(open(load_path, \"rb\"))\n",
        "else:\n",
        "    # Assuming best_params is defined somewhere in your code\n",
        "    model = VotingRegressor([\n",
        "        ('lgb_1', lgb.LGBMRegressor(**best_params, random_state=100)),\n",
        "        ('lgb_2', lgb.LGBMRegressor(**best_params, random_state=101)),\n",
        "        ('lgb_3', lgb.LGBMRegressor(**best_params, random_state=102)),\n",
        "        ('lgb_4', lgb.LGBMRegressor(**best_params, random_state=103)),\n",
        "        ('lgb_5', lgb.LGBMRegressor(**best_params, random_state=104)),\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)  # Calculate MAE\n",
        "\n",
        "# Format the print statements for large numbers\n",
        "print(f\"\\nMean Squared Error..: {mse:.2f}\")\n",
        "print(f\"R-squared...........: {r2:.4f}\")\n",
        "print(f\"Mean Absolute Error.: {mae:.2f}\")  # Print MAE\n",
        "\n",
        "if save_path is not None:\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLGiJ2hEdxDf",
        "outputId": "958e660d-a98a-431c-8809-20aa87d7d5fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:172: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Squared Error..: 104592.85\n",
            "R-squared...........: 0.8773\n",
            "Mean Absolute Error.: 123.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-Time Prediction Environment\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx7nIZyR9F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "bWbKTrs79F6O"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}