{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 57236,
          "databundleVersionId": 7230081,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-Energy-Comp/blob/main/enefit_project_FV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Description\n",
        "\n",
        "**Understand the specific problem of energy imbalance caused by prosumers and how the model can help Enefit**\n",
        "\n",
        "Specific Problem: Energy Imbalance Resulting from Prosumers\n",
        "\n",
        "The primary concern is the energy discrepancy that occurs when there is a disparity between the anticipated and the actual energy consumed or generated. The issue is worsened by prosumers contributing to the problem due to their simultaneous roles as energy consumers and producers. Their energy use and generation can be erratic, resulting in logistical and financial difficulties for energy firms like Enefit. These problems encompass the struggle to align supply and demand and the resulting expenditures from this imbalance.\n",
        "\n",
        "The Role of the Model in Facilitating Enefit\n",
        "The model aims to address these difficulties by offering precise forecasts of prosumers' energy usage and production. By doing so, the model will:\n",
        "\n",
        "1. Increase Forecasting Precision: Enhance Enefit's capacity to anticipate energy demands and production levels accurately.\n",
        "   \n",
        "2. Minimise Imbalance Expenses: Enefit can optimise its energy allocation by utilising more accurate forecasts, hence decreasing the expenses linked to energy imbalance.\n",
        "\n",
        "3. Enhance Resource Allocation Efficiency: Precise predictions will allow Enefit to distribute resources more optimally, thereby minimising waste and decreasing operational expenses.\n",
        "\n",
        "4. Enhance Strategic Decision-Making: Enefit can improve its ability to make strategic decisions on infrastructure investments and policy changes by gaining deeper insights into consumer behaviour.\n",
        "\n",
        "5. Encourage Sustainable Habits: Enefit may encourage prosumers to use renewable energy sources through efficient energy management, facilitating the shift towards more environmentally friendly energy habits.\n",
        "\n",
        "The model must incorporate multiple variables that impact consumer behaviour, such as weather patterns, past energy usage trends, pricing fluctuations, etc. The model's performance will be assessed based on its Mean Absolute Error (MAE), which requires your predictions to match the actual values to minimise the error measurement closely.\n",
        "\n",
        "The competition offers a dataset of historical meteorological data, energy pricing, and details regarding prosumer attributes. The given Python time-series API will guarantee that the model complies with the competition's specifications, including the prohibition of looking ahead in time and utilising just the available data for making predictions."
      ],
      "metadata": {
        "id": "iF04LaTg0ygt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup an Environment\n",
        "\n",
        "\n",
        "This code snippet is written in Python and includes a series of import statements, bringing various Python libraries and functions into the current script. These libraries are primarily used for data manipulation, machine learning, and optimization. Let us break down each part:\n",
        "\n",
        "1. **Import Statements**:\n",
        "   - `import os`: Imports the OS module, which provides functions for interacting with the operating system.\n",
        "   - `import gc`: Imports the garbage collector interface, which can manually trigger Python's garbage collection process.\n",
        "   - `import pickle`: Imports the pickle module used for serializing (pickling) and deserializing (unpickling) Python object structures.\n",
        "   - `import numpy as np`: Imports the NumPy library (as `np`), which is fundamental for scientific computing in Python, providing support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
        "   - `import pandas as pd`: Imports the pandas library (as `pd`), a powerful data manipulation and analysis tool.\n",
        "   - `import polars as pl`: Imports the Polars library (as `pl`), another data manipulation and analysis library, similar to pandas but often faster for certain operations.\n",
        "   - `from sklearn.model_selection import cross_val_score, cross_validate`: Imports `cross_val_score` and `cross_validate` functions from Scikit-Learn, used for cross-validation in machine learning.\n",
        "   - `from sklearn.metrics import mean_absolute_error`: Imports the Mean Absolute Error (MAE) metric from Scikit-Learn, used to evaluate the performance of regression models.\n",
        "   - `from sklearn.compose import TransformedTargetRegressor`: Imports a utility from Scikit-Learn to transform the target variable in regression problems.\n",
        "   - `from sklearn.ensemble import VotingRegressor`: Imports the VotingRegressor from Scikit-Learn, a meta-regressor that fits several base regressors and averages their predictions.\n",
        "   - `import lightgbm as lgb`: Imports LightGBM (as `lgb`), a gradient boosting framework that uses tree-based learning algorithms.\n",
        "   - `!pip install optuna`: Uses the pip package installer to install Optuna, a library for hyperparameter optimization.\n",
        "   - `import optuna`: Imports the Optuna library for optimizing machine learning models.\n",
        "\n",
        "2. **Purpose of the Code**:\n",
        "   - This code snippet creates an environment for data processing, machine learning, and hyperparameter optimization.\n",
        "   - It is likely part of a larger script or project focused on building, evaluating, and optimizing machine learning models, particularly regression models, given the import of `mean_absolute_error` and `TransformedTargetRegressor`.\n",
        "   - The use of `cross_val_score` and `cross_validate` suggests that cross-validation is used for model evaluation.\n",
        "   - `VotingRegressor` and `lightgbm` indicate that ensemble methods and gradient boosting are used in model training.\n",
        "   - `Optuna` is used to optimize the hyperparameters of these models to enhance performance.\n",
        "\n",
        "This code, by itself, doesn't perform any data analysis or machine learning operations. It is a setup for such tasks, defining the necessary libraries and tools to be used in subsequent code."
      ],
      "metadata": {
        "id": "UrdU4x2Lf_2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "!pip install optuna\n",
        "\n",
        "import optuna"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEWOCiWD9F6E",
        "outputId": "b7c830ba-5106-4804-c0e0-944d29961e5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validation Strategy\n",
        "\n",
        "The provided code snippet defines a Python class named `MonthlyKFold`, which appears to be a custom implementation of a cross-validation strategy. This class is designed to use time series data where observations are organized by month and year. Let us break down the code:\n",
        "\n",
        "1. **Class Definition - `MonthlyKFold`**:\n",
        "   - The class `MonthlyKFold` is intended for cross-validation in machine learning, specifically for datasets with a time component (monthly data in this case).\n",
        "\n",
        "2. **Constructor - `__init__(self, n_splits=3)`**:\n",
        "   - The `__init__` method is the class's constructor. It initializes the instance with the number of splits (`n_splits`) for cross-validation. By default, the splits are set to 3 if not specified.\n",
        "\n",
        "3. **Method - `split(self, X, y, groups=None)`**:\n",
        "   - The `split` method is the main functionality of this class. It is called with the features (`X`) and the target (`y'), along with an optional `groups` parameter.\n",
        "   - The method calculates a unique identifier for each month by combining the year and month values from the dataset. This assumes that `X` has columns named \"year\" and \"month\".\n",
        "   - It then sorts these unique monthly timesteps.\n",
        "   - The method iterates over the last `n_splits` months. For each of these months:\n",
        "     - It determines the indices of the training set (data before the current month) and the test set (data of the current month).\n",
        "     - It yields these indices, allowing for splitting the dataset into training and testing sets for each fold in the cross-validation.\n",
        "\n",
        "4. **Method - `get_n_splits(self, X, y, groups=None)`**:\n",
        "   - This method returns the number of splits (`n_splits`) the cross-validation process will use. This is a standard method in Scikit-Learn's cross-validator classes.\n",
        "\n",
        "5. **Usage and Purpose**:\n",
        "   - This custom cross-validator is particularly useful for time-series data where you want to ensure that the validation set comes chronologically after the training set, respecting the temporal order.\n",
        "   - The design suggests it has been used for scenarios where models must be evaluated based on their performance on unseen future data, a common requirement in time-series forecasting.\n",
        "\n",
        "In summary, `MonthlyKFold` is a custom class for performing time-based cross-validation, particularly suited for monthly data sets. It ensures that the model is continually trained on past data and tested on future data, adhering to the chronological order, which is crucial in time-series analysis."
      ],
      "metadata": {
        "id": "O21a-By7kvT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MonthlyKFold:\n",
        "    def __init__(self, n_splits=3):\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split(self, X, y, groups=None):\n",
        "        dates = 12 * X[\"year\"] + X[\"month\"]\n",
        "        timesteps = sorted(dates.unique().tolist())\n",
        "        X = X.reset_index()\n",
        "\n",
        "        for t in timesteps[-self.n_splits:]:\n",
        "            idx_train = X[dates.values < t].index\n",
        "            idx_test = X[dates.values == t].index\n",
        "\n",
        "            yield idx_train, idx_test\n",
        "\n",
        "    def get_n_splits(self, X, y, groups=None):\n",
        "        return self.n_splits\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "iK0lfj249F6H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "The provided code snippet defines a Python function named `to_pandas`, which is designed to convert datasets into a Pandas DataFrame and perform specific transformations on it. Let us break down the function:\n",
        "\n",
        "1. **Function Definition - `to_pandas(X, y=None)`**:\n",
        "   - The function `to_pandas` takes two arguments: `X` and an optional `y'. `X` is presumably a dataset, and `y' could be a target variable associated with `X`.\n",
        "   - The function is designed to work with datasets that have methods similar to the Pandas DataFrame (like `.to_pandas()`), suggesting that `X` and `y' might be in a format used by libraries like Polars, or they could be Pandas DataFrames already.\n",
        "\n",
        "2. **Converting to Pandas DataFrame**:\n",
        "   - If `y' is provided, the function concatenates `X` and `y' along the columns (axis=1) after converting them to Pandas DataFrames. This suggests that `X` and `y' are separate but related datasets, like features and target labels in machine learning.\n",
        "   - If `y' is not provided, it converts `X` to a Pandas DataFrame.\n",
        "\n",
        "3. **Data Transformation**:\n",
        "   - The function defines a list of categorical columns (`cat_cols`). It then sets the \"row_id\" column as the index of the DataFrame and converts the columns in `cat_cols` to the categorical data type. This is often done to optimize memory usage and improve performance in certain operations.\n",
        "   - It calculates the mean and standard deviation of a series of columns named \"target_1\" through \"target_6\" and creates new columns \"target_mean\" and \"target_std\" to store these values. This is typically done for feature engineering, where new features are derived from existing data.\n",
        "   - It also calculates a ratio of \"target_6\" to \"target_7\" (with a small constant added to the denominator to avoid division by zero) and stores this in a new column \"target_ratio\".\n",
        "\n",
        "4. **Return Value**:\n",
        "   - The function returns the transformed DataFrame.\n",
        "\n",
        "5. **Usage and Purpose**:\n",
        "   - This function is likely used in a data processing pipeline, especially in contexts where machine learning models are being developed.\n",
        "   - Its primary purpose is to prepare the data by converting it into a format suitable for analysis (Pandas DataFrame), setting the correct index, converting specific columns to categorical types for better processing, and creating new features valuable for predictive modelling.\n",
        "\n",
        "In summary, `to_pandas` is a utility function for data preparation, particularly in a machine-learning context. It transforms datasets by converting them into Pandas DataFrames, setting appropriate data types, and engineering new features."
      ],
      "metadata": {
        "id": "WwVpMVpHsRnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_pandas(X, y=None):\n",
        "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
        "\n",
        "    if y is not None:\n",
        "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
        "    else:\n",
        "        df = X.to_pandas()\n",
        "\n",
        "    df = df.set_index(\"row_id\")\n",
        "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
        "\n",
        "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
        "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
        "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
        "\n",
        "    return df\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "pkec2e4y9F6I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Optimization\n",
        "\n",
        "The provided code snippet defines a Python function named `lgb_objective`, which appears to be designed as an objective function for hyperparameter optimization using Optuna, specifically for a LightGBM regression model. Let us break down the function:\n",
        "\n",
        "1. **Function Definition - `lgb_objective(trial)`**:\n",
        "   - The function `lgb_objective` takes a single argument `trial`, representing an Optuna trial. An Optuna trial is used to suggest hyperparameters and evaluate their performance.\n",
        "\n",
        "2. **Hyperparameter Suggestions**:\n",
        "   - Inside the function, a dictionary named `params` holds the hyperparameters for a LightGBM model.\n",
        "   - The function uses `trial.suggest_*` methods to define a range of values for various hyperparameters. These methods are part of Optuna and are used to sample hyperparameters from specified ranges:\n",
        "     - `learning_rate`: A float between 0.01 and 0.1.\n",
        "     - `colsample_bytree` and `colsample_bynode`: Floats between 0.5 and 1.0, specifying the subsample ratio of columns when constructing each tree/node.\n",
        "     - `lambda_l1` and `lambda_l2`: Regularization parameters float between 0.01 and 10.0.\n",
        "     - `min_data_in_leaf`: An integer between 4 and 256, specifying the minimum number of samples per leaf node.\n",
        "     - `max_depth`: An integer between 5 and 10, specifying the maximum depth of the trees.\n",
        "     - `max_bin`: An integer between 32 and 1024, specifying the maximum number of bins used for constructing histograms.\n",
        "   - Fixed parameters like `n_iter`, `verbose`, `random_state`, and `objective` are also set.\n",
        "\n",
        "3. **Model Training and Cross-Validation**:\n",
        "   - A LightGBM regressor model is instantiated with the suggested parameters.\n",
        "   - The function assumes the existence of a DataFrame `df_train`, from which it separates the features (`X`) and the target variable (`y').\n",
        "   - It uses a previously defined `MonthlyKFold` object for cross-validation. This is likely the same `MonthlyKFold` class defined in a previous code snippet, which handles time-based splitting for cross-validation.\n",
        "   - The model's performance is evaluated using the `cross_val_score` function from Scikit-Learn, with the scoring metric set to 'neg_mean_absolute_error'.\n",
        "\n",
        "4. **Objective Value Return**:\n",
        "   - The function returns the negative mean of the cross-validation scores. In optimization, we often minimize a value, so returning the negative mean absolute error makes it a minimization problem. Optuna will aim to find hyperparameters that minimize this returned value.\n",
        "\n",
        "5. **Usage and Purpose**:\n",
        "   - This function is used in hyperparameter tuning using Optuna for a LightGBM regression model.\n",
        "   - It is designed to iteratively test different sets of hyperparameters, evaluate their performance using cross-validation, and return a score that reflects the average performance of a model with those hyperparameters.\n",
        "   - The Optuna framework uses this function to identify the best set of hyperparameters for the model based on the objective function's output.\n",
        "\n",
        "In summary, `lgb_objective` is an objective function for hyperparameter optimization of a LightGBM regression model, utilizing cross-validation and designed to be used with the Optuna hyperparameter optimization framework."
      ],
      "metadata": {
        "id": "wvmCA5FHvf2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_objective(trial):\n",
        "    params = {\n",
        "        'n_iter'           : 1000,\n",
        "        'verbose'          : -1,\n",
        "        'random_state'     : 42,\n",
        "        'objective'        : 'l2',\n",
        "        'learning_rate'    : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'colsample_bynode' : trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
        "        'lambda_l1'        : trial.suggest_float('lambda_l1', 1e-2, 10.0),\n",
        "        'lambda_l2'        : trial.suggest_float('lambda_l2', 1e-2, 10.0),\n",
        "        'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 4, 256),\n",
        "        'max_depth'        : trial.suggest_int('max_depth', 5, 10),\n",
        "        'max_bin'          : trial.suggest_int('max_bin', 32, 1024),\n",
        "    }\n",
        "\n",
        "    model  = lgb.LGBMRegressor(**params)\n",
        "    X, y   = df_train.drop(columns=[\"target\"]), df_train[\"target\"]\n",
        "    cv     = MonthlyKFold(1)\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
        "\n",
        "    return -1 * np.mean(scores)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UpjmRMGP9F6J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "This Python code defines several functions focused on data preprocessing, particularly for handling time series and geospatial data using the Polars library. It also includes a comprehensive function for feature engineering that integrates all these preprocessing steps. Let us break down each function and the overall workflow:\n",
        "\n",
        "1. **`convert_to_polars(*dfs)`**:\n",
        "   - Converts a list of data frames (possibly in different formats) to Polaris DataFrames. It checks if each dataframe is already a Polars DataFrame; if not, it converts it.\n",
        "\n",
        "2. **`process_datetime(df, column_name, is_date=False)`**:\n",
        "   - Processes datetime columns in a dataframe to ensure they are in the correct format (either `pl.Date` or `pl.Datetime`). If `is_date` is `True, ' it processes the column as a date; otherwise, it processes it as a datetime.\n",
        "\n",
        "3. **`process_location(df)`**:\n",
        "   - Converts latitude and longitude columns in a dataframe to `float32` type. This is useful for geospatial data processing.\n",
        "\n",
        "4. **`join_dataframes(df_main, dfs, join_conditions, suffixes)`**:\n",
        "   - Joins multiple data frames with specified conditions and suffixes. It iterates through a list of data frames (`dfs`), joining each to the main data frame (`df_main`) based on the join conditions. If the specified join condition columns are not present in the main and the joining data frame, the join is skipped for that data frame.\n",
        "\n",
        "5. **`add_time_features(df)`**:\n",
        "   - Adds time-related features to a dataframe, such as ordinal day, hour, day, weekday, month, year, and trigonometric transformations of day and hour. This is useful for capturing cyclical nature in time data.\n",
        "\n",
        "6. **`feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)`**:\n",
        "   - This is the primary feature engineering function. It takes multiple data frames as inputs, each representing different aspects of a dataset (like client information, gas usage, electricity usage, forecasts, historical data, location data, and target variables).\n",
        "   - Each dataframe is processed using the previously defined functions. Datetime columns are formatted correctly, location data is cast to float, and the data frames are joined based on specified conditions.\n",
        "   - Time-related features are added to the main dataframe.\n",
        "   - Unnecessary columns are optionally dropped at the end.\n",
        "   - The processed and feature-engineered main dataframe is returned.\n",
        "\n",
        "This code is a comprehensive approach to preprocessing and feature engineering for a dataset that combines multiple data sources, including time series and geospatial data. It is likely part of a more extensive data analysis or machine learning pipeline where such preprocessing is crucial for model training and analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "7MAmk-OPxcWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "\n",
        "def convert_to_polars(*dfs):\n",
        "    \"\"\"Converts a list of dataframes to Polars DataFrames.\"\"\"\n",
        "    return [pl.DataFrame(df) if not isinstance(df, pl.DataFrame) else df for df in dfs]\n",
        "\n",
        "def process_datetime(df, column_name, is_date=False):\n",
        "    \"\"\"Processes datetime columns to ensure correct format.\"\"\"\n",
        "    if column_name in df.columns:\n",
        "        if is_date and df.dtypes[df.columns.index(column_name)] == pl.Date:\n",
        "            df = df.with_columns(pl.col(column_name).cast(pl.Date))\n",
        "        elif not is_date and df.dtypes[df.columns.index(column_name)] == pl.Datetime:\n",
        "            df = df.with_columns(pl.col(column_name).cast(pl.Datetime))\n",
        "    return df\n",
        "\n",
        "def process_location(df):\n",
        "    \"\"\"Converts latitude and longitude to float.\"\"\"\n",
        "    return df.with_columns(\n",
        "        pl.col(\"latitude\").cast(pl.Float32),\n",
        "        pl.col(\"longitude\").cast(pl.Float32)\n",
        "    )\n",
        "\n",
        "def join_dataframes(df_main, dfs, join_conditions, suffixes):\n",
        "    \"\"\"Joins multiple dataframes with specified conditions and suffixes.\"\"\"\n",
        "    for df, condition, suffix in zip(dfs, join_conditions, suffixes):\n",
        "        if isinstance(condition, list):\n",
        "            condition_check = all(col in df_main.columns and col in df.columns for col in condition)\n",
        "        else:\n",
        "            condition_check = condition in df_main.columns and condition in df.columns\n",
        "\n",
        "        if condition_check:\n",
        "            df_main = df_main.join(df, on=condition, how=\"left\", suffix=suffix)\n",
        "        else:\n",
        "            print(f\"Skipping join for {suffix} due to missing column in condition: {condition}\")\n",
        "    return df_main\n",
        "\n",
        "def add_time_features(df):\n",
        "    \"\"\"Adds time-related features to the dataframe.\"\"\"\n",
        "    if \"datetime\" in df.columns and df.dtypes[df.columns.index(\"datetime\")] in [pl.Datetime, pl.Date]:\n",
        "        df = df.with_columns(\n",
        "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
        "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
        "            pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
        "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
        "            pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
        "            pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
        "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
        "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
        "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
        "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\")\n",
        "        )\n",
        "    else:\n",
        "        print(\"Warning: 'datetime' column not found. Time features cannot be added.\")\n",
        "    return df\n",
        "\n",
        "def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target):\n",
        "    # Convert to Polars DataFrame\n",
        "    df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target = convert_to_polars(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "\n",
        "    # Process each DataFrame\n",
        "    df_data = process_datetime(df_data, \"datetime\")\n",
        "    df_client = process_datetime(df_client, \"date\", is_date=True)\n",
        "    df_gas = process_datetime(df_gas, \"forecast_date\", is_date=True)\n",
        "    df_electricity = process_datetime(df_electricity, \"forecast_date\")\n",
        "    df_location = process_location(df_location)\n",
        "    df_forecast = process_datetime(df_forecast, \"forecast_datetime\")\n",
        "    df_historical = process_datetime(df_historical, \"datetime\")  # Assuming df_historical has a datetime column\n",
        "\n",
        "    # Define join conditions and suffixes\n",
        "    join_conditions = [\n",
        "        \"date\",\n",
        "        [\"county\", \"is_business\", \"product_type\", \"date\"],\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        [\"county\", \"datetime\"],\n",
        "        \"datetime\"\n",
        "    ]\n",
        "    suffixes = [\"_gas\", \"_client\", \"_elec\", \"_fcast\", \"_hist\", \"_loc\", \"_target\"]\n",
        "\n",
        "    # Join dataframes\n",
        "    df_data = join_dataframes(df_data, [df_gas, df_client, df_electricity, df_forecast, df_historical, df_location, df_target], join_conditions, suffixes)\n",
        "\n",
        "    # Add time-related features\n",
        "    df_data = add_time_features(df_data)\n",
        "\n",
        "    # Optionally, drop unnecessary columns\n",
        "    df_data = df_data.drop([\"date\", \"datetime\", \"hour\", \"dayofyear\"])\n",
        "\n",
        "    return df_data\n"
      ],
      "metadata": {
        "id": "UYofNURu_2qc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables\n",
        "\n",
        "The code snippet provided is a data preprocessing setup in a Python script, likely for a data analysis or machine learning project focused on energy behaviour prediction. It defines variables that specify file paths, column names for different datasets, and paths for saving or loading processed data. Let us break it down:\n",
        "\n",
        "1. **File Paths:**\n",
        "   - `root`: Defines the root directory where the data files are stored. It is currently set to a path in Google Drive, indicating that this script might be used in a Google Colab environment. The commented-out path suggests an alternative location, possibly for a Kaggle kernel.\n",
        "\n",
        "2. **Column Name Lists:**\n",
        "   - These lists define specific columns to be used or expected in different datasets. Each list corresponds to a different aspect of the data:\n",
        "     - `data_cols`: The main data columns, likely the primary dataset.\n",
        "     - `client_cols`: Columns related to client information.\n",
        "     - `gas_cols`: Columns related to gas price forecasts.\n",
        "     - `electricity_cols`: Columns related to electricity price forecasts.\n",
        "     - `forecast_cols`: Columns related to weather forecasts.\n",
        "     - `historical_cols`: Columns related to historical weather data.\n",
        "     - `location_cols`: Columns related to geographical location.\n",
        "     - `target_cols`: Columns related to target variables for prediction.\n",
        "\n",
        "3. **Save and Load Paths:**\n",
        "   - `save_path` and `load_path` are placeholders for paths where processed data can be saved or loaded. They are currently set to `None`, indicating that they will either be defined later in the script or that it currently does not use these functionalities.\n",
        "\n",
        "4. **Usage and Purpose:**\n",
        "   - This setup is typically used in data processing scripts where different datasets (like client data, energy forecasts, weather forecasts, historical weather data, etc.) are read, processed, and possibly joined or merged for analysis.\n",
        "   - The defined column lists help select, filter, or process specific parts of these datasets.\n",
        "   - The root path and the save/load paths are essential for file management in data processing workflows, especially when working with large datasets or in cloud environments like Google Colab.\n",
        "\n",
        "In summary, this code snippet is a preparatory part of a larger data processing script, setting up necessary variables for file paths and dataset columns, which will be used in subsequent data loading, preprocessing, and analysis steps in a project related to predicting energy behaviour."
      ],
      "metadata": {
        "id": "Gn9cdSMD9F6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
        "root = \"/content/drive/MyDrive/project_energy\"\n",
        "\n",
        "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
        "location_cols    = ['longitude', 'latitude', 'county']\n",
        "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
        "\n",
        "save_path = None\n",
        "load_path = None\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "zx4JfvPl9F6K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "3NWf6Ol49F6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports the 'drive' module from 'google.colab' and mounts the Google Drive to\n",
        "# the '/content/drive' directory in the Colab environment.\n",
        "from google.colab import drive\n",
        "\n",
        "# This function mounts Google Drive\n",
        "def mount_google_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function to mount Google Drive\n",
        "mount_google_drive()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5KJZJ3J-QNR",
        "outputId": "8aa30a79-7a53-4dd9-e0e0-2be4fa25a867"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data\n",
        "\n",
        "This code snippet is part of a data loading and schema inspection process in a Python script, using the Polars library to handle CSV files. It reads multiple datasets from CSV files, selects specific columns, and extracts their schemas. Let us go through each part of the code:\n",
        "\n",
        "1. **Reading CSV Files with Polars**:\n",
        "   - The code uses `pl.read_csv()` from the Polars library to read various CSV files. Each file represents a different aspect of the data related to energy behaviour prediction.\n",
        "   - `os.path.join(root, \"filename.csv\")` constructs the file path for each CSV file, using the `root` variable defined earlier as the base directory.\n",
        "   The `columns` parameter in `pl.read_csv()` specifies which columns to read from each CSV file. The relevant column names are provided by the variables defined in the previous code snippet (like `data_cols`, `client_cols`, etc.).\n",
        "   - `try_parse_dates=True` attempts to automatically parse columns recognized as date columns into appropriate date formats.\n",
        "\n",
        "2. **DataFrames for Different Data Aspects**:\n",
        "   - `df_data`, `df_client`, `df_gas`, `df_electricity`, `df_forecast`, `df_historical`, and `df_location` are the DataFrames created for the training data, client information, gas prices, electricity prices, weather forecast, historical weather, and location mapping, respectively.\n",
        "   - `df_target` is created by selecting target columns from `df_data` using the `select()` method.\n",
        "\n",
        "3. **Schema Inspection**:\n",
        "   - For each DataFrame, the code extracts its schema (data types of each column) and stores it in a corresponding schema variable (like `schema_data`, `schema_client`, etc.).\n",
        "   - The schema of a DataFrame provides information about the type of data in each column, which is crucial for data preprocessing and understanding the nature of the data.\n",
        "\n",
        "4. **Purpose and Usage**:\n",
        "   - This code is used for loading and initial inspection of various datasets that will likely be used in an energy behaviour prediction project.\n",
        "   - It ensures that the data is loaded with the correct columns and provides an initial look at the data types, essential for subsequent data processing and analysis steps.\n",
        "   - The separation of data into different frames based on their nature (like client data, gas prices, etc.) suggests a structured approach to handling a complex dataset with multiple facets.\n",
        "\n",
        "In summary, the code is part of a data loading and exploration process, focusing on reading different datasets related to energy behaviour, selecting specific columns, parsing date columns where possible, and inspecting the data types of each column for further processing and analysis."
      ],
      "metadata": {
        "id": "YBthTl6M2exN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data        = pl.read_csv(os.path.join(root, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
        "df_client      = pl.read_csv(os.path.join(root, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
        "df_gas         = pl.read_csv(os.path.join(root, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
        "df_electricity = pl.read_csv(os.path.join(root, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
        "df_forecast    = pl.read_csv(os.path.join(root, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
        "df_historical  = pl.read_csv(os.path.join(root, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
        "df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
        "df_target      = df_data.select(target_cols)\n",
        "\n",
        "schema_data        = df_data.schema\n",
        "schema_client      = df_client.schema\n",
        "schema_gas         = df_gas.schema\n",
        "schema_electricity = df_electricity.schema\n",
        "schema_forecast    = df_forecast.schema\n",
        "schema_historical  = df_historical.schema\n",
        "schema_target      = df_target.schema\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ntnXGXI39F6L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Data for ML\n",
        "\n",
        "The code provided continues the data processing and feature engineering workflow in Python, with an additional step for filtering the data and inspecting the resulting DataFrame. Let us break it down:\n",
        "\n",
        "1. **Separating Features and Target**:\n",
        "   - `X, y = df_data.drop(\"target\"), df_data.select(\"target\")`: This line is the same as in the previous snippet. It splits `df_data` into features (`X`) and target variables (`y').\n",
        "\n",
        "2. **Feature Engineering**:\n",
        "   - `X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)`: Also identical to the previous snippet, this line applies the `feature_eng` function to integrate and enrich the features from different data sources.\n",
        "\n",
        "3. **Converting to Pandas DataFrame**:\n",
        "   - `df_train = to_pandas(X, y)`: This line converts the feature-engineered data into a Pandas DataFrame for easier handling and analysis.\n",
        "\n",
        "4. **Filtering the DataFrame**:\n",
        "   - `df_train = df_train[df_train[\"target\"].notnull() & df_train[\"year\"].gt(2021)]`: This line filters out rows from `df_train` based on two conditions:\n",
        "     - Rows where the \"target\" column has null values are removed (`df_train[\"target\"].notnull()`).\n",
        "     - Only rows where the \"year\" column is greater than 2021 are kept (`df_train[\"year\"].gt(2021)`).\n",
        "   - This filtering is likely done to ensure the model trains only on complete and recent data, a common practice to improve model performance and relevance.\n",
        "\n",
        "5. **Inspecting the DataFrame**:\n",
        "   - `df_train.info(verbose=True)`: This line prints detailed information about `df_train`, including the number of non-null entries in each column, the data type of each column, and memory usage.\n",
        "   - This is a valuable step for getting an overview of the final dataset before proceeding to model training or further analysis.\n",
        "\n",
        "6. **Purpose and Usage**:\n",
        "   - This code snippet is part of the data preparation process in a machine learning or data analysis workflow. It is focused on ensuring that the data is rich in features, clean (accessible of null values), and relevant (consisting of recent data).\n",
        "   - The inspection of the DataFrame helps in understanding the data structure, which is crucial before moving into model training or in-depth analysis.\n",
        "\n",
        "In summary, the code finalizes the data preparation by filtering the dataset to remove null values and focus on recent data. It also inspects the final dataset to confirm its readiness for further steps in the machine learning or data analysis workflow."
      ],
      "metadata": {
        "id": "feLGWmSh9F6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
        "\n",
        "X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "\n",
        "df_train = to_pandas(X, y)\n",
        "df_train = df_train[df_train[\"target\"].notnull() & df_train[\"year\"].gt(2021)]\n",
        "df_train.info(verbose=True)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk3CrqpS9F6M",
        "outputId": "80a361fc-fa58-44e1-d7e0-3febeffb19fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping join for _gas due to missing column in condition: date\n",
            "Skipping join for _client due to missing column in condition: ['county', 'is_business', 'product_type', 'date']\n",
            "Skipping join for _elec due to missing column in condition: datetime\n",
            "Skipping join for _fcast due to missing column in condition: datetime\n",
            "Skipping join for _loc due to missing column in condition: datetime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "The provided code snippet defines a Python dictionary named `best_params`, which appears to contain a set of optimized hyperparameters for a machine learning model, specifically a LightGBM model, given the nature of the parameters. Let us go through each key-value pair in the dictionary:\n",
        "\n",
        "1. **`'n_iter': 900`**:\n",
        "   - Specifies the number of iterations for the model training process. In the context of LightGBM, this typically refers to the number of boosting rounds.\n",
        "\n",
        "2. **`'verbose': -1`**:\n",
        "   - Sets the verbosity level for the model's training process. A value of `-1` generally means that the process will be silent, i.e., no logs will be shown during training.\n",
        "\n",
        "3. **`'objective': 'l2'`**:\n",
        "   - Indicates the objective function to be used by the model. Here, `'l2'` refers to the L2 loss, also known as mean squared error, commonly used for regression tasks.\n",
        "\n",
        "4. **`'learning_rate': 0.05689066836106983`**:\n",
        "   - This is the learning rate of the model, a crucial hyperparameter in gradient-boosting models. It determines the step size at each iteration while moving towards a minimum of the loss function.\n",
        "\n",
        "5. **`'colsample_bytree': 0.8915976762048253`**:\n",
        "   - Specifies the subsample ratio of columns when constructing each tree. Values closer to 1 mean more columns are used to build each tree.\n",
        "\n",
        "6. **`'colsample_bynode': 0.5942203285139224`**:\n",
        "   - This parameter is similar to `colsample_bytree` but applies to each node of the trees, specifying the subsample ratio of columns for each split.\n",
        "\n",
        "7. **`'lambda_l1': 3.6277555139102864`** and **`'lambda_l2': 1.6591278779517808`**:\n",
        "   - These represent L1 (Lasso) and L2 (Ridge) regularization terms. They are used to prevent overfitting by adding penalties to the model.\n",
        "\n",
        "8. **`'min_data_in_leaf': 186`**:\n",
        "   - Defines the minimum number of data points required to form a leaf. This can be used to control overfitting.\n",
        "\n",
        "9. **`'max_depth': 9`**:\n",
        "   - Specifies the maximum depth of each tree. Deeper trees can model more complex patterns but can also lead to overfitting.\n",
        "\n",
        "10. **`'max_bin': 813`**:\n",
        "    - Determines the maximum number of bins used for bucketing feature values. Higher numbers allow the algorithm to consider more split points, potentially leading to more accurate models but increasing computation.\n",
        "\n",
        "The dictionary `best_params` suggests that these parameters were likely obtained through a hyperparameter tuning process, possibly using a tool like Optuna, as indicated in previous code snippets. These optimized parameters are usually used to configure a LightGBM model to achieve better performance on a specific dataset. The exact values are tailored to the data's characteristics and the machine-learning task's specific requirements."
      ],
      "metadata": {
        "id": "pgM4VhGW9F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {\n",
        "    'n_iter'           : 900,\n",
        "    'verbose'          : -1,\n",
        "    'objective'        : 'l2',\n",
        "    'learning_rate'    : 0.05689066836106983,\n",
        "    'colsample_bytree' : 0.8915976762048253,\n",
        "    'colsample_bynode' : 0.5942203285139224,\n",
        "    'lambda_l1'        : 3.6277555139102864,\n",
        "    'lambda_l2'        : 1.6591278779517808,\n",
        "    'min_data_in_leaf' : 186,\n",
        "    'max_depth'        : 9,\n",
        "    'max_bin'          : 813,\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "UzMaXRoU9F6N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "\n",
        "The code snippet provided is part of a machine-learning workflow in Python, focused explicitly on model loading, training, and saving. It deals with either loading a pre-trained model or training a new one and optionally saving it. Let us dissect each part:\n",
        "\n",
        "1. **Loading a Pre-trained Model (if available)**:\n",
        "   - The first `if` statement checks whether `load_path` is not `None`. If there is a specified `load_path`, it assumes a pre-trained model is saved at this location.\n",
        "   - `model = pickle.load(open(load_path, \"rb\"))`: This line uses the `pickle` module to load a serialized model from the specified file path. The `\"rb\"` argument indicates that the file is opened in read-binary mode.\n",
        "\n",
        "2. **Creating and Training a New Model (if no pre-trained model)**:\n",
        "   - If `load_path` is `None`, a new model is created using the `VotingRegressor` from Scikit-Learn. This model is an ensemble of several LightGBM regressors.\n",
        "   - Each LightGBM regressor (`lgb.LGBMRegressor`) is instantiated with the same `best_params` (optimized hyperparameters) but different `random_state` values. This diversity in random states helps create slightly varied models, which can benefit an ensemble.\n",
        "   - The `VotingRegressor` aggregates the predictions of each regressor to make more robust overall predictions.\n",
        "   - The model is then trained on the `df_train` DataFrame using the `fit` method. Features (`X`) are obtained by dropping the `\"target\"` column, and the target variable (`y') is the `\"target\"` column.\n",
        "\n",
        "3. **Saving the Trained Model (if specified)**:\n",
        "   - After training (or loading) the model, the `if` statement checks if `save_path` is not `None`.\n",
        "   - If a `save_path` is provided, the trained model is serialized and saved to this path using `pickle.dump`. The `\"wb\"` argument indicates that the file is opened in write-binary mode.\n",
        "\n",
        "4. **Purpose and Usage**:\n",
        "   - This code snippet is a crucial part of a machine-learning pipeline where model persistence is essential. It provides flexibility to either load a pre-trained model (useful for scenarios like model deployment or when retraining is not required) or train a new model from scratch.\n",
        "   - Saving the trained model allows the user to reuse the model later without retraining it, saving time and computational resources.\n",
        "\n",
        "In summary, the code manages a machine learning model's loading, training, and saving, specifically a Voting Regressor ensemble of LightGBM models. This allows for efficient model reuse and persistence in a machine-learning workflow."
      ],
      "metadata": {
        "id": "BMRSXOv39F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if load_path is not None:\n",
        "    model = pickle.load(open(load_path, \"rb\"))\n",
        "else:\n",
        "    model = VotingRegressor([\n",
        "        ('lgb_1', lgb.LGBMRegressor(**best_params, random_state=100)),\n",
        "        ('lgb_2', lgb.LGBMRegressor(**best_params, random_state=101)),\n",
        "        ('lgb_3', lgb.LGBMRegressor(**best_params, random_state=102)),\n",
        "        ('lgb_4', lgb.LGBMRegressor(**best_params, random_state=103)),\n",
        "        ('lgb_5', lgb.LGBMRegressor(**best_params, random_state=104)),\n",
        "    ])\n",
        "\n",
        "    model.fit(\n",
        "        X=df_train.drop(columns=[\"target\"]),\n",
        "        y=df_train[\"target\"]\n",
        "    )\n",
        "\n",
        "if save_path is not None:\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "iKaH3Phl9F6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-Time Prediction Environment\n",
        "\n",
        "This code snippet is part of a machine-learning workflow designed for a competition or a real-time prediction environment, on a platform like Kaggle. It uses an iterative testing approach, common in time-series forecasting competitions, where the model is used to make predictions on test data as it becomes available. Let us break down the critical components of the code:\n",
        "\n",
        "1. **Import and Environment Setup**:\n",
        "   - `import enefit`: This line imports a module named `enefit`, which is likely specific to the context or competition for which this code is written.\n",
        "   - `env = enefit.make_env()`: Creates an environment for the iterative test process. This environment provides test data in chunks over time.\n",
        "\n",
        "2. **Iterative Testing Loop**:\n",
        "   - `iter_test = env.iter_test()`: Initializes an iterator for the test set.\n",
        "   - The `for` loop iterates over the test data provided by `iter_test`. Each iteration yields several DataFrames representing different aspects of the test data, such as `test`, `client`, `historical_weather`, etc.\n",
        "\n",
        "3. **Data Processing**:\n",
        "   - `test = test.rename(columns={\"prediction_datetime\": \"datetime\"})`: Renames a column in the `test` DataFrame for consistency.\n",
        "   - Several DataFrames (`df_test`, `df_client`, `df_gas`, etc.) are created from the test data chunks using Polars, with column selections based on predefined schemas.\n",
        "   - `df_forecast`, `df_historical`, and `df_target` are updated by concatenating new data and removing duplicates.\n",
        "\n",
        "4. **Feature Engineering and Prediction**:\n",
        "   - `X_test = feature_eng(...)`: Applies the previously defined `feature_eng` function to process and combine the test data.\n",
        "   - `X_test = to_pandas(X_test)`: Converts the processed test data to a Pandas DataFrame.\n",
        "   - `sample_prediction[\"target\"] = model.predict(X_test).clip(0)`: The model makes predictions on the test data. The `.clip(0)` method ensures that no negative predictions are made, which might be important depending on the context (e.g., predicting quantities that cannot be negative).\n",
        "\n",
        "5. **Submitting Predictions**:\n",
        "   - `env.predict(sample_prediction)`: Submits the predictions for the current test batch.\n",
        "\n",
        "6. **Purpose and Usage**:\n",
        "   - This code is typically used in a competition or real-world scenario where predictions are made as new data becomes available, often in a time-series forecasting context.\n",
        "   - The iterative approach allows the model to use the most recent data for making predictions, which can be crucial for accuracy in time-sensitive contexts.\n",
        "\n",
        "In summary, the code is set up for an iterative prediction environment, processing incoming test data, applying feature engineering, making predictions with a trained model, and submitting these predictions. The exact context (like the nature of the `enefit` library and the data involved) is specific to the particular use case or competition for which this code is written."
      ],
      "metadata": {
        "id": "Xx7nIZyR9F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enefit\n",
        "\n",
        "env = enefit.make_env()\n",
        "iter_test = env.iter_test()\n",
        "\n",
        "for (test, revealed_targets, client, historical_weather,\n",
        "        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
        "\n",
        "    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
        "\n",
        "    df_test           = pl.from_pandas(test[data_cols[1:]], schema_overrides=schema_data)\n",
        "    df_client         = pl.from_pandas(client[client_cols], schema_overrides=schema_client)\n",
        "    df_gas            = pl.from_pandas(gas_prices[gas_cols], schema_overrides=schema_gas)\n",
        "    df_electricity    = pl.from_pandas(electricity_prices[electricity_cols], schema_overrides=schema_electricity)\n",
        "    df_new_forecast   = pl.from_pandas(forecast_weather[forecast_cols], schema_overrides=schema_forecast)\n",
        "    df_new_historical = pl.from_pandas(historical_weather[historical_cols], schema_overrides=schema_historical)\n",
        "    df_new_target     = pl.from_pandas(revealed_targets[target_cols], schema_overrides=schema_target)\n",
        "\n",
        "    df_forecast       = pl.concat([df_forecast, df_new_forecast]).unique()\n",
        "    df_historical     = pl.concat([df_historical, df_new_historical]).unique()\n",
        "    df_target         = pl.concat([df_target, df_new_target]).unique()\n",
        "\n",
        "    X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "    X_test = to_pandas(X_test)\n",
        "\n",
        "    sample_prediction[\"target\"] = model.predict(X_test).clip(0)\n",
        "\n",
        "    env.predict(sample_prediction)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bWbKTrs79F6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}