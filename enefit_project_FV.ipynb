{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 57236,
          "databundleVersionId": 7230081,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-Energy-Comp/blob/main/enefit_project_FV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up an Environment for Data Processing\n",
        "\n",
        "This code snippet is written in Python and includes a series of import statements, bringing various Python libraries and functions into the current script. These libraries are primarily used for data manipulation, machine learning, and optimization. Let us break down each part:\n",
        "\n",
        "1. Import Statements:\n",
        "   - `import os`: Imports the OS module, which provides functions for interacting with the operating system.\n",
        "   - `import gc`: Imports the garbage collector interface, which can manually trigger Python's garbage collection process.\n",
        "   - `import pickle`: Imports the pickle module used for serializing (pickling) and deserializing (unpickling) Python object structures.\n",
        "   - `import numpy as np`: Imports the NumPy library (as `np`), which is fundamental for scientific computing in Python, providing support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
        "   - `import pandas as pd`: Imports the pandas library (as `pd`), a powerful data manipulation and analysis tool.\n",
        "   - `import polars as pl`: Imports the Polars library (as `pl`), another data manipulation and analysis library, similar to pandas but often faster for certain operations.\n",
        "   - `from sklearn.model_selection import cross_val_score, cross_validate`: Imports `cross_val_score` and `cross_validate` functions from Scikit-Learn, used for cross-validation in machine learning.\n",
        "   - `from sklearn.metrics import mean_absolute_error`: Imports the Mean Absolute Error (MAE) metric from Scikit-Learn, used to evaluate the performance of regression models.\n",
        "   - `from sklearn.compose import TransformedTargetRegressor`: Imports a utility from Scikit-Learn to transform the target variable in regression problems.\n",
        "   - `from sklearn.ensemble import VotingRegressor`: Imports the VotingRegressor from Scikit-Learn, a meta-regressor that fits several base regressors and averages their predictions.\n",
        "   - `import lightgbm as lgb`: Imports LightGBM (as `lgb`), a gradient boosting framework that uses tree-based learning algorithms.\n",
        "   - `!pip install optuna`: Uses the pip package installer to install Optuna, a library for hyperparameter optimization.\n",
        "   - `import optuna`: Imports the Optuna library for optimizing machine learning models.\n",
        "\n",
        "2. Purpose of the Code:\n",
        "   - This code snippet creates an environment for data processing, machine learning, and hyperparameter optimization.\n",
        "   - It is likely part of a larger script or project focused on building, evaluating, and optimizing machine learning models, particularly regression models, given the import of `mean_absolute_error` and `TransformedTargetRegressor`.\n",
        "   - The use of `cross_val_score` and `cross_validate` suggests that cross-validation is used for model evaluation.\n",
        "   - `VotingRegressor` and `lightgbm` indicate that ensemble methods and gradient boosting are used in model training.\n",
        "   - `Optuna` is used to optimize the hyperparameters of these models to enhance performance.\n",
        "\n",
        "This code, by itself, doesn't perform any data analysis or machine learning operations. It is a setup for such tasks, defining the necessary libraries and tools to be used in subsequent code."
      ],
      "metadata": {
        "id": "UrdU4x2Lf_2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "!pip install optuna\n",
        "\n",
        "import optuna"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEWOCiWD9F6E",
        "outputId": "f3436759-2252-4954-8f94-b90cc3237138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.0 colorlog-6.8.0 optuna-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MonthlyKFold:\n",
        "    def __init__(self, n_splits=3):\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split(self, X, y, groups=None):\n",
        "        dates = 12 * X[\"year\"] + X[\"month\"]\n",
        "        timesteps = sorted(dates.unique().tolist())\n",
        "        X = X.reset_index()\n",
        "\n",
        "        for t in timesteps[-self.n_splits:]:\n",
        "            idx_train = X[dates.values < t].index\n",
        "            idx_test = X[dates.values == t].index\n",
        "\n",
        "            yield idx_train, idx_test\n",
        "\n",
        "    def get_n_splits(self, X, y, groups=None):\n",
        "        return self.n_splits"
      ],
      "metadata": {
        "trusted": true,
        "id": "iK0lfj249F6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_pandas(X, y=None):\n",
        "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
        "\n",
        "    if y is not None:\n",
        "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
        "    else:\n",
        "        df = X.to_pandas()\n",
        "\n",
        "    df = df.set_index(\"row_id\")\n",
        "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
        "\n",
        "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
        "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
        "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "id": "pkec2e4y9F6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_objective(trial):\n",
        "    params = {\n",
        "        'n_iter'           : 1000,\n",
        "        'verbose'          : -1,\n",
        "        'random_state'     : 42,\n",
        "        'objective'        : 'l2',\n",
        "        'learning_rate'    : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'colsample_bynode' : trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
        "        'lambda_l1'        : trial.suggest_float('lambda_l1', 1e-2, 10.0),\n",
        "        'lambda_l2'        : trial.suggest_float('lambda_l2', 1e-2, 10.0),\n",
        "        'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 4, 256),\n",
        "        'max_depth'        : trial.suggest_int('max_depth', 5, 10),\n",
        "        'max_bin'          : trial.suggest_int('max_bin', 32, 1024),\n",
        "    }\n",
        "\n",
        "    model  = lgb.LGBMRegressor(**params)\n",
        "    X, y   = df_train.drop(columns=[\"target\"]), df_train[\"target\"]\n",
        "    cv     = MonthlyKFold(1)\n",
        "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
        "\n",
        "    return -1 * np.mean(scores)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UpjmRMGP9F6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "\n",
        "def convert_to_polars(*dfs):\n",
        "    \"\"\"Converts a list of dataframes to Polars DataFrames.\"\"\"\n",
        "    return [pl.DataFrame(df) if not isinstance(df, pl.DataFrame) else df for df in dfs]\n",
        "\n",
        "def process_datetime(df, column_name, is_date=False):\n",
        "    \"\"\"Processes datetime columns to ensure correct format.\"\"\"\n",
        "    if column_name in df.columns:\n",
        "        if is_date and df.dtypes[df.columns.index(column_name)] == pl.Date:\n",
        "            df = df.with_columns(pl.col(column_name).cast(pl.Date))\n",
        "        elif not is_date and df.dtypes[df.columns.index(column_name)] == pl.Datetime:\n",
        "            df = df.with_columns(pl.col(column_name).cast(pl.Datetime))\n",
        "    return df\n",
        "\n",
        "def process_location(df):\n",
        "    \"\"\"Converts latitude and longitude to float.\"\"\"\n",
        "    return df.with_columns(\n",
        "        pl.col(\"latitude\").cast(pl.Float32),\n",
        "        pl.col(\"longitude\").cast(pl.Float32)\n",
        "    )\n",
        "\n",
        "def join_dataframes(df_main, dfs, join_conditions, suffixes):\n",
        "    \"\"\"Joins multiple dataframes with specified conditions and suffixes.\"\"\"\n",
        "    for df, condition, suffix in zip(dfs, join_conditions, suffixes):\n",
        "        if isinstance(condition, list):\n",
        "            condition_check = all(col in df_main.columns and col in df.columns for col in condition)\n",
        "        else:\n",
        "            condition_check = condition in df_main.columns and condition in df.columns\n",
        "\n",
        "        if condition_check:\n",
        "            df_main = df_main.join(df, on=condition, how=\"left\", suffix=suffix)\n",
        "        else:\n",
        "            print(f\"Skipping join for {suffix} due to missing column in condition: {condition}\")\n",
        "    return df_main\n",
        "\n",
        "def add_time_features(df):\n",
        "    \"\"\"Adds time-related features to the dataframe.\"\"\"\n",
        "    if \"datetime\" in df.columns and df.dtypes[df.columns.index(\"datetime\")] in [pl.Datetime, pl.Date]:\n",
        "        df = df.with_columns(\n",
        "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
        "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
        "            pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
        "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
        "            pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
        "            pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
        "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
        "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
        "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
        "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\")\n",
        "        )\n",
        "    else:\n",
        "        print(\"Warning: 'datetime' column not found. Time features cannot be added.\")\n",
        "    return df\n",
        "\n",
        "def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target):\n",
        "    # Convert to Polars DataFrame\n",
        "    df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target = convert_to_polars(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "\n",
        "    # Process each DataFrame\n",
        "    df_data = process_datetime(df_data, \"datetime\")\n",
        "    df_client = process_datetime(df_client, \"date\", is_date=True)\n",
        "    df_gas = process_datetime(df_gas, \"forecast_date\", is_date=True)\n",
        "    df_electricity = process_datetime(df_electricity, \"forecast_date\")\n",
        "    df_location = process_location(df_location)\n",
        "    df_forecast = process_datetime(df_forecast, \"forecast_datetime\")\n",
        "    df_historical = process_datetime(df_historical, \"datetime\")  # Assuming df_historical has a datetime column\n",
        "\n",
        "    # Define join conditions and suffixes\n",
        "    join_conditions = [\n",
        "        \"date\",\n",
        "        [\"county\", \"is_business\", \"product_type\", \"date\"],\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        \"datetime\",\n",
        "        [\"county\", \"datetime\"],\n",
        "        \"datetime\"\n",
        "    ]\n",
        "    suffixes = [\"_gas\", \"_client\", \"_elec\", \"_fcast\", \"_hist\", \"_loc\", \"_target\"]\n",
        "\n",
        "    # Join dataframes\n",
        "    df_data = join_dataframes(df_data, [df_gas, df_client, df_electricity, df_forecast, df_historical, df_location, df_target], join_conditions, suffixes)\n",
        "\n",
        "    # Add time-related features\n",
        "    df_data = add_time_features(df_data)\n",
        "\n",
        "    # Optionally, drop unnecessary columns\n",
        "    df_data = df_data.drop([\"date\", \"datetime\", \"hour\", \"dayofyear\"])\n",
        "\n",
        "    return df_data\n"
      ],
      "metadata": {
        "id": "UYofNURu_2qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Variables"
      ],
      "metadata": {
        "id": "Gn9cdSMD9F6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
        "root = \"/content/drive/MyDrive/project_energy\"\n",
        "\n",
        "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
        "location_cols    = ['longitude', 'latitude', 'county']\n",
        "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
        "\n",
        "save_path = None\n",
        "load_path = None"
      ],
      "metadata": {
        "trusted": true,
        "id": "zx4JfvPl9F6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data I/O"
      ],
      "metadata": {
        "id": "3NWf6Ol49F6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports the 'drive' module from 'google.colab' and mounts the Google Drive to\n",
        "# the '/content/drive' directory in the Colab environment.\n",
        "from google.colab import drive\n",
        "\n",
        "# This function mounts Google Drive\n",
        "def mount_google_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function to mount Google Drive\n",
        "mount_google_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5KJZJ3J-QNR",
        "outputId": "7180d1b1-be73-4f48-f57c-c2ab0ac9d683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data        = pl.read_csv(os.path.join(root, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
        "df_client      = pl.read_csv(os.path.join(root, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
        "df_gas         = pl.read_csv(os.path.join(root, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
        "df_electricity = pl.read_csv(os.path.join(root, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
        "df_forecast    = pl.read_csv(os.path.join(root, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
        "df_historical  = pl.read_csv(os.path.join(root, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
        "df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
        "df_target      = df_data.select(target_cols)\n",
        "\n",
        "schema_data        = df_data.schema\n",
        "schema_client      = df_client.schema\n",
        "schema_gas         = df_gas.schema\n",
        "schema_electricity = df_electricity.schema\n",
        "schema_forecast    = df_forecast.schema\n",
        "schema_historical  = df_historical.schema\n",
        "schema_target      = df_target.schema"
      ],
      "metadata": {
        "trusted": true,
        "id": "ntnXGXI39F6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "feLGWmSh9F6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
        "\n",
        "X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "\n",
        "df_train = to_pandas(X, y)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk3CrqpS9F6M",
        "outputId": "3ebaa7fe-f7c6-4b6f-fd0a-561fb686e1e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping join for _gas due to missing column in condition: date\n",
            "Skipping join for _client due to missing column in condition: ['county', 'is_business', 'product_type', 'date']\n",
            "Skipping join for _elec due to missing column in condition: datetime\n",
            "Skipping join for _fcast due to missing column in condition: datetime\n",
            "Skipping join for _loc due to missing column in condition: datetime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[df_train[\"target\"].notnull() & df_train[\"year\"].gt(2021)]"
      ],
      "metadata": {
        "trusted": true,
        "id": "UF5sAGdt9F6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info(verbose=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "K1hZVOOA9F6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HyperParam Optimization"
      ],
      "metadata": {
        "id": "pgM4VhGW9F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# study = optuna.create_study(direction='minimize', study_name='Regressor')\n",
        "# study.optimize(lgb_objective, n_trials=100, show_progress_bar=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tBL6Uh6m9F6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {\n",
        "    'n_iter'           : 900,\n",
        "    'verbose'          : -1,\n",
        "    'objective'        : 'l2',\n",
        "    'learning_rate'    : 0.05689066836106983,\n",
        "    'colsample_bytree' : 0.8915976762048253,\n",
        "    'colsample_bynode' : 0.5942203285139224,\n",
        "    'lambda_l1'        : 3.6277555139102864,\n",
        "    'lambda_l2'        : 1.6591278779517808,\n",
        "    'min_data_in_leaf' : 186,\n",
        "    'max_depth'        : 9,\n",
        "    'max_bin'          : 813,\n",
        "} # val score is 62.24 for the last month"
      ],
      "metadata": {
        "trusted": true,
        "id": "UzMaXRoU9F6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "KkhxyBR19F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''result = cross_validate(\n",
        "    estimator=lgb.LGBMRegressor(**best_params, random_state=42),\n",
        "    X=df_train.drop(columns=[\"target\"]),\n",
        "    y=df_train[\"target\"],\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    cv=MonthlyKFold(1),\n",
        ")\n",
        "\n",
        "print(f\"Fit Time(s): {result['fit_time'].mean():.3f}\")\n",
        "print(f\"Score Time(s): {result['score_time'].mean():.3f}\")\n",
        "print(f\"Error(MAE): {-result['test_score'].mean():.3f}\")'''"
      ],
      "metadata": {
        "trusted": true,
        "id": "MkQIqU-39F6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "BMRSXOv39F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if load_path is not None:\n",
        "    model = pickle.load(open(load_path, \"rb\"))\n",
        "else:\n",
        "    model = VotingRegressor([\n",
        "        ('lgb_1', lgb.LGBMRegressor(**best_params, random_state=100)),\n",
        "        ('lgb_2', lgb.LGBMRegressor(**best_params, random_state=101)),\n",
        "        ('lgb_3', lgb.LGBMRegressor(**best_params, random_state=102)),\n",
        "        ('lgb_4', lgb.LGBMRegressor(**best_params, random_state=103)),\n",
        "        ('lgb_5', lgb.LGBMRegressor(**best_params, random_state=104)),\n",
        "    ])\n",
        "\n",
        "    model.fit(\n",
        "        X=df_train.drop(columns=[\"target\"]),\n",
        "        y=df_train[\"target\"]\n",
        "    )\n",
        "\n",
        "if save_path is not None:\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(model, f)"
      ],
      "metadata": {
        "trusted": true,
        "id": "iKaH3Phl9F6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "Xx7nIZyR9F6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enefit\n",
        "\n",
        "env = enefit.make_env()\n",
        "iter_test = env.iter_test()"
      ],
      "metadata": {
        "trusted": true,
        "id": "SEzzXlLm9F6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (test, revealed_targets, client, historical_weather,\n",
        "        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
        "\n",
        "    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
        "\n",
        "    df_test           = pl.from_pandas(test[data_cols[1:]], schema_overrides=schema_data)\n",
        "    df_client         = pl.from_pandas(client[client_cols], schema_overrides=schema_client)\n",
        "    df_gas            = pl.from_pandas(gas_prices[gas_cols], schema_overrides=schema_gas)\n",
        "    df_electricity    = pl.from_pandas(electricity_prices[electricity_cols], schema_overrides=schema_electricity)\n",
        "    df_new_forecast   = pl.from_pandas(forecast_weather[forecast_cols], schema_overrides=schema_forecast)\n",
        "    df_new_historical = pl.from_pandas(historical_weather[historical_cols], schema_overrides=schema_historical)\n",
        "    df_new_target     = pl.from_pandas(revealed_targets[target_cols], schema_overrides=schema_target)\n",
        "\n",
        "    df_forecast       = pl.concat([df_forecast, df_new_forecast]).unique()\n",
        "    df_historical     = pl.concat([df_historical, df_new_historical]).unique()\n",
        "    df_target         = pl.concat([df_target, df_new_target]).unique()\n",
        "\n",
        "    X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
        "    X_test = to_pandas(X_test)\n",
        "\n",
        "    sample_prediction[\"target\"] = model.predict(X_test).clip(0)\n",
        "\n",
        "    env.predict(sample_prediction)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bWbKTrs79F6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}