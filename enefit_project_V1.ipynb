{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 57236,
          "databundleVersionId": 7230081,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-Energy-Comp/blob/main/enefit_project_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries and Packages"
      ],
      "metadata": {
        "id": "PavxL879uDXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor  # or RandomForestClassifier for classification\n",
        "from sklearn.metrics import mean_absolute_error     # Import MAE instead of MSE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "!pip install optuna\n",
        "\n",
        "import optuna\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEWOCiWD9F6E",
        "outputId": "d7406258-8d3f-40a4-acd1-df77ee73204b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.0 colorlog-6.8.0 optuna-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "3NWf6Ol49F6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports the 'drive' module from 'google.colab' and mounts the Google Drive to\n",
        "# the '/content/drive' directory in the Colab environment.\n",
        "from google.colab import drive\n",
        "\n",
        "# This function mounts Google Drive\n",
        "def mount_google_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Call the function to mount Google Drive\n",
        "mount_google_drive()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5KJZJ3J-QNR",
        "outputId": "d27c6562-2771-4ae4-93bd-e035fee99dd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "jV_fVYcPt9fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
        "root = \"/content/drive/MyDrive/project_energy\"\n",
        "\n",
        "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
        "location_cols    = ['longitude', 'latitude', 'county']\n",
        "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
        "\n",
        "save_path = None\n",
        "load_path = None\n",
        "\n",
        "df_data        = pl.read_csv(os.path.join(root, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
        "df_client      = pl.read_csv(os.path.join(root, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
        "df_gas         = pl.read_csv(os.path.join(root, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
        "df_electricity = pl.read_csv(os.path.join(root, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
        "df_forecast    = pl.read_csv(os.path.join(root, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
        "df_historical  = pl.read_csv(os.path.join(root, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
        "df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
        "df_target      = df_data.select(target_cols)\n",
        "\n",
        "schema_data        = df_data.schema\n",
        "schema_client      = df_client.schema\n",
        "schema_gas         = df_gas.schema\n",
        "schema_electricity = df_electricity.schema\n",
        "schema_forecast    = df_forecast.schema\n",
        "schema_historical  = df_historical.schema\n",
        "schema_target      = df_target.schema\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "zx4JfvPl9F6K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering\n"
      ],
      "metadata": {
        "id": "8feX-mt_xvHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns specifications\n",
        "data_cols = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
        "client_cols = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
        "gas_cols = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
        "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
        "forecast_cols = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
        "historical_cols = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_high', 'windspeed_10m', 'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation', 'diffuse_radiation', 'latitude', 'longitude']\n",
        "location_cols = ['longitude', 'latitude', 'county']\n",
        "\n",
        "# Helper function to read large CSV files directly into Polars DataFrames\n",
        "def read_csv_polars(file_path, columns, batch_size=5 * 10 ** 5):  # Adjust batch_size as needed\n",
        "    return pl.read_csv(file_path, columns=columns, batch_size=batch_size)\n",
        "\n",
        "# Reading CSV files directly into Polars DataFrames\n",
        "df_data = read_csv_polars(os.path.join(root, \"train.csv\"), data_cols)\n",
        "df_client = read_csv_polars(os.path.join(root, \"client.csv\"), client_cols)\n",
        "df_gas = read_csv_polars(os.path.join(root, \"gas_prices.csv\"), gas_cols)\n",
        "df_electricity = read_csv_polars(os.path.join(root, \"electricity_prices.csv\"), electricity_cols)\n",
        "df_forecast = read_csv_polars(os.path.join(root, \"forecast_weather.csv\"), forecast_cols)\n",
        "df_historical = read_csv_polars(os.path.join(root, \"historical_weather.csv\"), historical_cols)\n",
        "df_location = read_csv_polars(os.path.join(root, \"weather_station_to_county_mapping.csv\"), location_cols)\n",
        "\n",
        "# Define a date format string compatible with your data\n",
        "date_format = \"%Y-%m-%d %H:%M:%S\"  # Adjust the format according to your data\n",
        "\n",
        "# Convert 'datetime' columns to datetime format in all dataframes\n",
        "df_data = df_data.with_columns([pl.col('datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_client = df_client.with_columns([pl.col('date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_gas = df_gas.with_columns([pl.col('forecast_date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_electricity = df_electricity.with_columns([pl.col('forecast_date').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_forecast = df_forecast.with_columns([pl.col('forecast_datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "df_historical = df_historical.with_columns([pl.col('datetime').str.strptime(pl.Datetime, date_format, strict=False)])\n",
        "\n",
        "# Standardize datetime precision to microseconds\n",
        "# This function is simplified assuming your datetime data is already timezone-naive\n",
        "def standardize_datetime_precision(df, datetime_columns):\n",
        "    for col in datetime_columns:\n",
        "        df = df.with_columns([pl.col(col).cast(pl.Datetime)])\n",
        "    return df\n",
        "\n",
        "datetime_columns_data = ['datetime']\n",
        "datetime_columns_client = ['date']\n",
        "datetime_columns_gas = ['forecast_date']\n",
        "datetime_columns_electricity = ['forecast_date']\n",
        "datetime_columns_forecast = ['forecast_datetime']\n",
        "datetime_columns_historical = ['datetime']\n",
        "\n",
        "df_data = standardize_datetime_precision(df_data, datetime_columns_data)\n",
        "df_client = standardize_datetime_precision(df_client, datetime_columns_client)\n",
        "df_gas = standardize_datetime_precision(df_gas, datetime_columns_gas)\n",
        "df_electricity = standardize_datetime_precision(df_electricity, datetime_columns_electricity)\n",
        "df_forecast = standardize_datetime_precision(df_forecast, datetime_columns_forecast)\n",
        "df_historical = standardize_datetime_precision(df_historical, datetime_columns_historical)\n",
        "\n",
        "# Filtering data for years greater than 2021\n",
        "df_data = df_data.filter(pl.col('datetime').dt.year() > 2021)\n",
        "\n",
        "# Function to convert data into a pandas DataFrame\n",
        "def to_pandas(X, y=None):\n",
        "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
        "\n",
        "    if y is not None:\n",
        "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
        "    else:\n",
        "        df = X.to_pandas()\n",
        "\n",
        "    df = df.set_index(\"row_id\")\n",
        "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
        "\n",
        "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
        "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
        "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Define the feature engineering function\n",
        "def feature_eng(df):\n",
        "    # Example: Creating a new feature by combining existing ones\n",
        "    if 'installed_capacity' in df.columns and 'eic_count' in df.columns:\n",
        "        df = df.with_columns([(pl.col('installed_capacity') * pl.col('eic_count')).alias('capacity_eic_product')])\n",
        "\n",
        "    # Example: Transforming a feature (e.g., logarithmic transformation)\n",
        "    if 'euros_per_mwh' in df.columns:\n",
        "        df = df.with_columns([pl.col('euros_per_mwh').log().alias('log_euros_per_mwh')])\n",
        "\n",
        "    # Example: Encoding categorical variables\n",
        "    if 'county' in df.columns:\n",
        "        df = df.join(df.groupby('county').count(), on='county', how='left').rename({'count': 'county_frequency'})\n",
        "\n",
        "    # Additional feature engineering steps...\n",
        "    # TO DO...\n",
        "\n",
        "    return df\n",
        "\n",
        "# Feature engineering and data preparation in batches\n",
        "number_of_batches = 100  # Adjust based on your memory capacity\n",
        "for batch in np.array_split(df_data.to_pandas(), number_of_batches):\n",
        "    X = pl.DataFrame(batch.drop(columns=[\"target\"]))\n",
        "    y = pl.DataFrame(batch[[\"target\"]])\n",
        "\n",
        "    # Apply feature engineering\n",
        "    X = feature_eng(X)\n",
        "\n",
        "# Garbage collection to free up memory\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "mk3CrqpS9F6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc50542b-a01c-48b6-be24-f2245528aed9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X is a Polars DataFrame, convert it to a Pandas DataFrame\n",
        "X_df = X.to_pandas()\n",
        "\n",
        "# Convert the 'datetime' column to a numeric format (e.g., seconds since epoch)\n",
        "X_df['datetime'] = X_df['datetime'].apply(pd.Timestamp.timestamp)\n",
        "\n",
        "# Data Preprocessing\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_df)\n",
        "\n",
        "# Splitting Data\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y.to_pandas(), test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert y_train and y_test to 1D array\n",
        "y_train = y_train.values.ravel()\n",
        "y_test = y_test.values.ravel()\n",
        "\n",
        "# Model Training\n",
        "# Initialize and train the model\n",
        "model = RandomForestRegressor(random_state=42)  # Use RandomForestClassifier for classification tasks\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the mean absolute error\n",
        "mae = mean_absolute_error(y_test, y_pred)  # Calculate MAE\n",
        "print(f\"\\n>> Mean Absolute Error: {mae}\")\n",
        "\n",
        "# Hyperparameter Tuning (Optional)\n",
        "# This can be done using GridSearchCV or RandomizedSearchCV from sklearn.model_selection\n",
        "\n",
        "# Prediction\n",
        "# Use model.predict(new_data) to make predictions on new, unseen data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OTGNkr1yjBU",
        "outputId": "3c7ba2ab-98b3-494c-d02c-80c249efc694"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">> Mean Absolute Error: 59.44933381845689\n"
          ]
        }
      ]
    }
  ]
}